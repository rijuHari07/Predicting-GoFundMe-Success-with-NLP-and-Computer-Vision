{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_DDBr8ndxNu"
   },
   "source": [
    "# Assignment 3 - Is a Picture Worth a Thousand Words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbKRzAeTeGMz"
   },
   "source": [
    "Team Members:\n",
    "- Abigail Peck (ap69393)\n",
    "- Emily Caraher (ec45335)\n",
    "- Nandini Anand Kumar (na29337)\n",
    "- Riju Hariharan (rh42988)\n",
    "- Sarvesh Miskin (sm88728)\n",
    "- Varsha Ramesh (vr23656)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIesQ5STeMuj"
   },
   "source": [
    "You are an analytics consultant to a non-profit organization (e.g., a pet rescue society, a childrenâ€™s hospital, etc.). Your objective is to help the organization raise more funds in its gofundme.com campaigns.\\\n",
    "\\\n",
    "Task A: Scrape ~1000 fundraising campaigns in a particular category (e.g., Golden Retriever rescue society, an organization raising funds for wildfire victims, etc.). Fetch (i) image URLs, (ii) text description of a post, (iii) $ raised, (iv) how long the campaign has been running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-hn2xLJOdvfR"
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# GoFundMe Business Category Scraper\n",
    "# Scrapes campaigns from https://www.gofundme.com/discover/business-fundraiser\n",
    "# \"\"\"\n",
    "\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# import pandas as pd\n",
    "# import time\n",
    "# import re\n",
    "# from datetime import datetime\n",
    "# from bs4 import BeautifulSoup\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# class GoFundMeBusinessScraper:\n",
    "#     def __init__(self, headless=True):\n",
    "#         \"\"\"Initialize the scraper with Selenium\"\"\"\n",
    "#         chrome_options = Options()\n",
    "#         if headless:\n",
    "#             chrome_options.add_argument(\"--headless\")\n",
    "#         chrome_options.add_argument(\"--no-sandbox\")\n",
    "#         chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "#         chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "#         chrome_options.add_argument(\"--disable-gpu\")\n",
    "#         chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "#         chrome_options.add_argument(\"--disable-extensions\")\n",
    "#         chrome_options.add_argument(\"--proxy-server='direct://'\")\n",
    "#         chrome_options.add_argument(\"--proxy-bypass-list=*\")\n",
    "#         chrome_options.add_argument(\"--start-maximized\")\n",
    "#         chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\")\n",
    "\n",
    "#         # Prevent crashes\n",
    "#         chrome_options.add_argument(\"--disable-software-rasterizer\")\n",
    "#         chrome_options.add_argument(\"--disable-dev-tools\")\n",
    "#         chrome_options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "\n",
    "#         self.driver = webdriver.Chrome(options=chrome_options)\n",
    "#         self.driver.set_page_load_timeout(30)\n",
    "#         self.campaigns = []\n",
    "#         self.base_url = \"https://www.gofundme.com/discover/business-fundraiser\"\n",
    "\n",
    "#     def click_show_more(self, max_clicks=50):\n",
    "#         \"\"\"Click the 'Show More' button to load more campaigns\"\"\"\n",
    "#         clicks = 0\n",
    "#         while clicks < max_clicks:\n",
    "#             try:\n",
    "#                 # Scroll to bottom first to make button visible\n",
    "#                 self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "#                 time.sleep(1)\n",
    "\n",
    "#                 # Find and click the show more button\n",
    "#                 show_more_button = self.driver.find_element(\n",
    "#                     By.CSS_SELECTOR,\n",
    "#                     \"button.hrt-secondary-button.hrt-base-button\"\n",
    "#                 )\n",
    "\n",
    "#                 # Scroll button into view and click\n",
    "#                 self.driver.execute_script(\"arguments[0].scrollIntoView(true);\", show_more_button)\n",
    "#                 time.sleep(1)\n",
    "#                 show_more_button.click()\n",
    "\n",
    "#                 clicks += 1\n",
    "#                 print(f\"Clicked 'Show More' button {clicks}/{max_clicks} times\")\n",
    "#                 time.sleep(2)  # Wait for content to load\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 print(f\"No more 'Show More' button found or error: {str(e)}\")\n",
    "#                 break\n",
    "\n",
    "#         print(f\"Finished loading campaigns after {clicks} clicks\")\n",
    "\n",
    "#     def scrape_campaign_cards(self, max_campaigns=1000):\n",
    "#         \"\"\"Scrape campaign cards from the discover page\"\"\"\n",
    "#         print(f\"Loading {self.base_url}\")\n",
    "#         self.driver.get(self.base_url)\n",
    "\n",
    "#         # Wait for page to load\n",
    "#         time.sleep(5)\n",
    "\n",
    "#         # Click \"Show More\" button to load more campaigns\n",
    "#         # Starts with 24 campaigns, adds 12 per click\n",
    "#         # Formula: 24 + (12 * num_clicks) >= max_campaigns\n",
    "#         # num_clicks = (max_campaigns - 24) / 12\n",
    "#         num_clicks = max(0, int((max_campaigns - 24) / 12) + 2)  # +2 for buffer\n",
    "#         print(f\"Need approximately {num_clicks} clicks to load {max_campaigns} campaigns\")\n",
    "#         self.click_show_more(max_clicks=num_clicks)\n",
    "\n",
    "#         # Find all campaign cards using the specific class\n",
    "#         print(\"Finding campaign cards...\")\n",
    "#         campaign_elements = self.driver.find_elements(By.CSS_SELECTOR, \"a.hrt-action-card.hrt-base-button\")\n",
    "#         print(f\"Found {len(campaign_elements)} campaigns\")\n",
    "\n",
    "#         # Extract unique campaign URLs\n",
    "#         campaign_urls = []\n",
    "#         for elem in campaign_elements:\n",
    "#             try:\n",
    "#                 url = elem.get_attribute('href')\n",
    "#                 if url and '/f/' in url and url not in campaign_urls:\n",
    "#                     campaign_urls.append(url)\n",
    "#             except:\n",
    "#                 continue\n",
    "\n",
    "#         print(f\"Processing {min(len(campaign_urls), max_campaigns)} unique campaigns\")\n",
    "\n",
    "#         # Scrape each campaign (limit to max_campaigns)\n",
    "#         for i, url in enumerate(campaign_urls[:max_campaigns]):\n",
    "#             print(f\"Scraping campaign {i+1}/{min(len(campaign_urls), max_campaigns)}: {url}\")\n",
    "\n",
    "#             # Try scraping with retry logic\n",
    "#             retries = 3\n",
    "#             for attempt in range(retries):\n",
    "#                 try:\n",
    "#                     campaign_data = self.scrape_campaign_page(url)\n",
    "#                     if campaign_data:\n",
    "#                         self.campaigns.append(campaign_data)\n",
    "#                     break\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Attempt {attempt+1} failed: {str(e)}\")\n",
    "#                     if attempt < retries - 1:\n",
    "#                         print(\"Retrying...\")\n",
    "#                         time.sleep(3)\n",
    "#                     else:\n",
    "#                         print(f\"Skipping campaign after {retries} attempts\")\n",
    "\n",
    "#             time.sleep(2)  # Be polite\n",
    "\n",
    "#         return self.campaigns\n",
    "\n",
    "#     def scrape_campaign_page(self, url):\n",
    "#         \"\"\"Scrape individual campaign page with reliable image and full description support\"\"\"\n",
    "#         try:\n",
    "#             # Restart driver if crashed\n",
    "#             try:\n",
    "#                 _ = self.driver.current_url\n",
    "#             except:\n",
    "#                 print(\"Browser crashed, reinitializing...\")\n",
    "#                 self.__init__(headless=True)\n",
    "#                 time.sleep(2)\n",
    "\n",
    "#             self.driver.get(url)\n",
    "#             time.sleep(3)\n",
    "\n",
    "#             campaign_data = {\n",
    "#                 'url': url,\n",
    "#                 'title': None,\n",
    "#                 'image_url': None,\n",
    "#                 'description': None,\n",
    "#                 'amount_raised': None,\n",
    "#                 'days_running': None\n",
    "#             }\n",
    "\n",
    "#             # Title\n",
    "#             try:\n",
    "#                 h1 = next(h for h in self.driver.find_elements(By.TAG_NAME, \"h1\") if h.text.strip())\n",
    "#                 campaign_data['title'] = h1.text.strip()\n",
    "#             except:\n",
    "#                 pass\n",
    "\n",
    "#             # Image\n",
    "#             try:\n",
    "#                 img = self.driver.find_element(By.TAG_NAME, 'img')\n",
    "#                 campaign_data['image_url'] = img.get_attribute('src')\n",
    "#             except:\n",
    "#                 print(f\"Image scrape failed\")\n",
    "\n",
    "#             # Full description from nested divs inside campaign-description container\n",
    "#             try:\n",
    "#                 desc_root = self.driver.find_element(By.CSS_SELECTOR, 'div.campaign-description_campaignDescription__6P_RU')\n",
    "#                 html = desc_root.get_attribute('innerHTML')\n",
    "#                 soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "#                 # Remove the \"Read more\" button to exclude its text\n",
    "#                 button = soup.find('button', {'data-element-id': 'btn_story_read-more'})\n",
    "#                 if button:\n",
    "#                     button.decompose()\n",
    "\n",
    "#                 # Replace <br> tags with newlines\n",
    "#                 for br in soup.find_all(\"br\"):\n",
    "#                     br.replace_with(\"\\n\")\n",
    "\n",
    "#                 # Extract only visible text, ignoring nested duplication\n",
    "#                 description = soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "#                 campaign_data['description'] = description\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Description scrape failed: {e}\")\n",
    "\n",
    "#             # Amount raised\n",
    "#             try:\n",
    "#                 body_text = self.driver.find_element(By.TAG_NAME, 'body').text\n",
    "#                 match = re.search(r'\\$([0-9,]+)\\s*raised', body_text, re.IGNORECASE)\n",
    "#                 if match:\n",
    "#                     campaign_data['amount_raised'] = match.group(1).replace(',', '')\n",
    "#             except:\n",
    "#                 pass\n",
    "\n",
    "#             # Created date and days running\n",
    "#             try:\n",
    "#                 date_elem = self.driver.find_element(By.CSS_SELECTOR, 'span.a-created-date')\n",
    "#                 raw_date = date_elem.text.strip().replace('Created', '').strip()\n",
    "#                 campaign_data['days_running'] = self.calculate_days_since(raw_date)\n",
    "#             except:\n",
    "#                 pass\n",
    "\n",
    "#             return campaign_data\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error scraping {url}: {e}\")\n",
    "#             return None\n",
    "\n",
    "#     def calculate_days_since(self, date_text):\n",
    "#         \"\"\"Calculate days since a given date string or relative phrase\"\"\"\n",
    "#         try:\n",
    "#             text = date_text.lower()\n",
    "\n",
    "#             # Handle relative formats\n",
    "#             if 'hour' in text and 'ago' in text:\n",
    "#                 match = re.search(r'(\\d+)\\s*hours?\\s*ago', text, re.IGNORECASE)\n",
    "#                 if match:\n",
    "#                     return int(match.group(1)) / 24  # Convert hours to days\n",
    "\n",
    "#             if ('day' in text or 'd ago' in text) and 'ago' in text:\n",
    "#                 match = re.search(r'(\\d+)\\s*d(?:ays?)?\\s*ago', text, re.IGNORECASE)\n",
    "#                 if match:\n",
    "#                     return int(match.group(1))\n",
    "\n",
    "#             if 'month' in text and 'ago' in text:\n",
    "#                 match = re.search(r'(\\d+)\\s*months?\\s*ago', text, re.IGNORECASE)\n",
    "#                 if match:\n",
    "#                     return int(match.group(1)) * 30\n",
    "\n",
    "#             if 'year' in text and 'ago' in text:\n",
    "#                 match = re.search(r'(\\d+)\\s*years?\\s*ago', text, re.IGNORECASE)\n",
    "#                 if match:\n",
    "#                     return int(match.group(1)) * 365\n",
    "\n",
    "#             # Handle absolute date like \"September 11th, 2025\"\n",
    "#             # Remove ordinal suffixes: st, nd, rd, th\n",
    "#             clean_date = re.sub(r'(\\d+)(st|nd|rd|th)', r'\\1', date_text)\n",
    "\n",
    "#             # Try parsing full date format\n",
    "#             try:\n",
    "#                 date_obj = datetime.strptime(clean_date.strip(), '%B %d, %Y')\n",
    "#             except ValueError:\n",
    "#                 # Try alternative format if needed\n",
    "#                 date_obj = datetime.strptime(clean_date.strip(), '%b %d, %Y')  # e.g., \"Sep 11, 2025\"\n",
    "\n",
    "#             today = datetime.now()\n",
    "#             return (today - date_obj).days\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Could not parse date: {date_text}, error: {e}\")\n",
    "#             return None\n",
    "\n",
    "#     def save_to_csv(self, filename='business_campaigns.csv'):\n",
    "#         \"\"\"Save to CSV\"\"\"\n",
    "#         df = pd.DataFrame(self.campaigns)\n",
    "#         df.to_csv(filename, index=False)\n",
    "#         print(f\"\\nSaved {len(self.campaigns)} campaigns to {filename}\")\n",
    "#         return df\n",
    "\n",
    "#     def close(self):\n",
    "#         \"\"\"Close the browser\"\"\"\n",
    "#         self.driver.quit()\n",
    "\n",
    "# # Usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Create scraper instance\n",
    "#     scraper = GoFundMeBusinessScraper(headless=False)  # Set to True to hide browser\n",
    "#     try:\n",
    "#         # Scrape detailed campaign data by visiting each page\n",
    "#         print(\"Scraping detailed campaign data by visiting each page...\")\n",
    "#         campaigns = scraper.scrape_campaign_cards(max_campaigns=1000)\n",
    "\n",
    "#         # Save to CSV\n",
    "#         df = scraper.save_to_csv('business_campaigns.csv')\n",
    "\n",
    "#         # Display summary\n",
    "#         print(f\"\\n{'='*50}\")\n",
    "#         print(f\"Total campaigns scraped: {len(campaigns)}\")\n",
    "#         print(f\"{'='*50}\")\n",
    "#     finally:\n",
    "#         scraper.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F88Qy_XreZA-"
   },
   "source": [
    "Task B: Using the image URLs, obtain image labels (text) from Google Vision (cloud service) or other services such as Azure. You can also use an LLM through its API. You will need an account, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Qc0Ct9WeZk5"
   },
   "outputs": [],
   "source": [
    "# pip install pandas requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZj_QDrvc6mQ"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import requests\n",
    "# import json\n",
    "# import time\n",
    "\n",
    "# API_KEY = \"AIzaSyD2MAHZ5gmPMN_SL06zxHBylAIJdfKJ_Z0\"  # Replace with your actual API key\n",
    "# VISION_URL = f\"https://vision.googleapis.com/v1/images:annotate?key=AIzaSyD2MAHZ5gmPMN_SL06zxHBylAIJdfKJ_Z0\"\n",
    "\n",
    "# # Load your CSV\n",
    "# df = pd.read_csv(\"business_campaigns.csv\")\n",
    "\n",
    "# labels_list = []\n",
    "\n",
    "# for idx, url in enumerate(df['image_url']):\n",
    "#     print(f\"Processing image {idx+1}/{len(df)}\")\n",
    "#     # Build request payload\n",
    "#     request_body = {\n",
    "#         \"requests\": [\n",
    "#             {\n",
    "#                 \"image\": {\n",
    "#                     \"source\": {\n",
    "#                         \"imageUri\": url\n",
    "#                     }\n",
    "#                 },\n",
    "#                 \"features\": [\n",
    "#                     {\n",
    "#                         \"type\": \"LABEL_DETECTION\",\n",
    "#                         \"maxResults\": 5\n",
    "#                     }\n",
    "#                 ]\n",
    "#             }\n",
    "#         ]\n",
    "#     }\n",
    "\n",
    "#     try:\n",
    "#         response = requests.post(VISION_URL, json=request_body)\n",
    "#         result = response.json()\n",
    "\n",
    "#         # Extract labels\n",
    "#         labels = result['responses'][0].get('labelAnnotations', [])\n",
    "#         label_descriptions = [label['description'] for label in labels]\n",
    "#         labels_list.append(\", \".join(label_descriptions))\n",
    "\n",
    "#         # Respect API limits\n",
    "#         time.sleep(0.2)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing image: {e}\")\n",
    "#         labels_list.append(\"\")\n",
    "\n",
    "# # Add results to dataframe\n",
    "# df['labels'] = labels_list\n",
    "\n",
    "# # Save to new CSV\n",
    "# df.to_csv(\"labeled_images.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ieFpCXFnNQ_"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# files.download(\"labeled_images.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ADfyaaUo2nR"
   },
   "source": [
    "Upload labeled_images.csv here - contains the following columns: url, title, image_url, description, amount_raised, days_running, and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "UBwgG2UWnNE5",
    "outputId": "361dc4ec-4d2d-425e-a063-eed17e2327a5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-4b0e1d9b-910c-47eb-9feb-e78d217afe22\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-4b0e1d9b-910c-47eb-9feb-e78d217afe22\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving labeled_images.csv to labeled_images.csv\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "altu6-kNebii"
   },
   "source": [
    "Task C: Create a column called binary (lowercase only) where value =1 (stands for high $$) or 0 (stands for low $) based on the median value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rcfJ-PpEAuKJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8pg0UzeHebQZ"
   },
   "outputs": [],
   "source": [
    "df_labeled = pd.read_csv('labeled_images.csv')\n",
    "df_labeled['amount_raised'] = pd.to_numeric(df_labeled['amount_raised'], errors='coerce')\n",
    "median_raised = df_labeled['amount_raised'].median()\n",
    "df_labeled['binary'] = df_labeled['amount_raised'].apply(lambda x: 1 if x >= median_raised else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IUi6nOvea13"
   },
   "source": [
    "Task D: Run a logistic regression with binary as the dependent variable, and the image_labels as independent variables. You can use a BoW model for text. Figure out how you can use the duration of the campaign in the model. What is the accuracy (show the confusion matrix) of this prediction model? The idea is to be able to predict the engagement level for an image.\\\n",
    "\\\n",
    "Accuracy = 1 - # prediction errors / total # cases\\\n",
    "\\\n",
    "What accuracy do you get by using the description words as the independent variables instead of image_labels? Finally, what accuracy do you get by combining (concatenating) the image_labels and word descriptions and using them together as independent variables? What can you conclude from your analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6q0oCVCyeawS",
    "outputId": "b46cbce0-1376-4744-e8ab-95d2ddc484d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image-label + Duration model accuracy: 0.735\n",
      "Confusion matrix:\n",
      " [[82 23]\n",
      " [30 65]]\n",
      "\n",
      "Description-text + Duration model accuracy : 0.735\n",
      "Confusion matrix:\n",
      " [[81 24]\n",
      " [29 66]]\n",
      "\n",
      " Image-label + Description-text + Duration model accuracy: 0.74\n",
      "Confusion matrix:\n",
      " [[82 23]\n",
      " [29 66]]\n",
      "\n",
      " Without Duration\n",
      "\n",
      "--- Model: Image Labels ONLY ---\n",
      "Accuracy: 0.72\n",
      "Confusion matrix:\n",
      " [[88 17]\n",
      " [39 56]]\n",
      "\n",
      "--- Model: Description Text ONLY ---\n",
      "Accuracy: 0.695\n",
      "Confusion matrix:\n",
      " [[80 25]\n",
      " [36 59]]\n",
      "\n",
      "--- Model: Image Labels + Description Text ONLY ---\n",
      "Accuracy: 0.69\n",
      "Confusion matrix:\n",
      " [[78 27]\n",
      " [35 60]]\n"
     ]
    }
   ],
   "source": [
    "#bag of words for image labels\n",
    "df_labeled['labels'] = df_labeled['labels'].fillna(\"\") #if no image labels changes from nan to an empty string\n",
    "vectorizer_img = CountVectorizer()\n",
    "X_img = vectorizer_img.fit_transform(df_labeled['labels'])\n",
    "\n",
    "#bag of words for description text\n",
    "vectorizer_txt = CountVectorizer(max_features=1000, stop_words='english')\n",
    "X_text = vectorizer_txt.fit_transform(df_labeled['description'].fillna('')) # Fill NaN descriptions\n",
    "\n",
    "#Scale and Normalize days running of each fundraiser\n",
    "# Fill NaN values in 'days_running' with the median before scaling\n",
    "df_labeled['days_running'] = df_labeled['days_running'].fillna(df_labeled['days_running'].median())\n",
    "X_duration = np.array(df_labeled['days_running']).reshape(-1, 1)\n",
    "scaler = StandardScaler()\n",
    "X_duration_scaled = scaler.fit_transform(X_duration)\n",
    "\n",
    "#dependent variable\n",
    "y = df_labeled['binary']\n",
    "\n",
    "#split the train and testing data\n",
    "#only split the data once and use the same indices for each independent variable\n",
    "X_train_idx, X_test_idx = train_test_split(df_labeled.index, test_size=0.2, random_state=42)\n",
    "X_train_img, X_test_img = X_img[X_train_idx], X_img[X_test_idx]\n",
    "X_train_txt, X_test_txt = X_text[X_train_idx], X_text[X_test_idx]\n",
    "X_train_dur, X_test_dur = X_duration_scaled[X_train_idx], X_duration_scaled[X_test_idx]\n",
    "y_train, y_test = y[X_train_idx], y[X_test_idx]\n",
    "\n",
    "\n",
    "# Combine sparse and dense features\n",
    "X_train_img_full = hstack([X_train_img, X_train_dur])\n",
    "X_test_img_full  = hstack([X_test_img, X_test_dur])\n",
    "\n",
    "logreg = LogisticRegression(max_iter=5000)\n",
    "logreg.fit(X_train_img_full, y_train)\n",
    "\n",
    "y_pred_img = logreg.predict(X_test_img_full)\n",
    "acc_img = accuracy_score(y_test, y_pred_img)\n",
    "cm_img = confusion_matrix(y_test, y_pred_img)\n",
    "\n",
    "print(\"Image-label + Duration model accuracy:\", round(acc_img, 3))\n",
    "print(\"Confusion matrix:\\n\", cm_img)\n",
    "\n",
    "\n",
    "# Model with Description Text + Days Running\n",
    "X_train_txt_full = hstack([X_train_txt, X_train_dur])\n",
    "X_test_txt_full  = hstack([X_test_txt, X_test_dur])\n",
    "\n",
    "logreg_txt = LogisticRegression(max_iter=5000)\n",
    "logreg_txt.fit(X_train_txt_full, y_train)\n",
    "\n",
    "y_pred_txt = logreg_txt.predict(X_test_txt_full)\n",
    "acc_txt = accuracy_score(y_test, y_pred_txt)\n",
    "cm_txt = confusion_matrix(y_test, y_pred_txt)\n",
    "\n",
    "print(\"\\nDescription-text + Duration model accuracy :\", round(acc_txt, 3))\n",
    "print(\"Confusion matrix:\\n\", cm_txt)\n",
    "\n",
    "# Model with Combined Features (Image Labels + Description Text + Days Running)\n",
    "X_train_combined = hstack([X_train_img, X_train_txt, X_train_dur])\n",
    "X_test_combined  = hstack([X_test_img, X_test_txt, X_test_dur])\n",
    "\n",
    "logreg_combined = LogisticRegression(max_iter=5000)\n",
    "logreg_combined.fit(X_train_combined, y_train)\n",
    "\n",
    "y_pred_combined = logreg_combined.predict(X_test_combined)\n",
    "acc_combined = accuracy_score(y_test, y_pred_combined)\n",
    "cm_combined = confusion_matrix(y_test, y_pred_combined)\n",
    "\n",
    "print(\"\\n Image-label + Description-text + Duration model accuracy:\", round(acc_combined, 3))\n",
    "print(\"Confusion matrix:\\n\", cm_combined)\n",
    "\n",
    "print(\"\\n Without Duration\")\n",
    "logreg_img_only = LogisticRegression(max_iter=5000)\n",
    "logreg_img_only.fit(X_train_img, y_train)\n",
    "\n",
    "# --- Predict and Evaluate ---\n",
    "y_pred_img_only = logreg_img_only.predict(X_test_img)\n",
    "acc_img_only = accuracy_score(y_test, y_pred_img_only)\n",
    "cm_img_only = confusion_matrix(y_test, y_pred_img_only)\n",
    "\n",
    "# --- Print Results ---\n",
    "print(\"\\n--- Model: Image Labels ONLY ---\")\n",
    "print(\"Accuracy:\", round(acc_img_only, 4))\n",
    "print(\"Confusion matrix:\\n\", cm_img_only)\n",
    "\n",
    "logreg_txt_only = LogisticRegression(max_iter=5000)\n",
    "logreg_txt_only.fit(X_train_txt, y_train)\n",
    "\n",
    "# --- Predict and Evaluate ---\n",
    "y_pred_txt_only = logreg_txt_only.predict(X_test_txt)\n",
    "acc_txt_only = accuracy_score(y_test, y_pred_txt_only)\n",
    "cm_txt_only = confusion_matrix(y_test, y_pred_txt_only)\n",
    "\n",
    "# --- Print Results ---\n",
    "print(\"\\n--- Model: Description Text ONLY ---\")\n",
    "print(\"Accuracy:\", round(acc_txt_only, 4))\n",
    "print(\"Confusion matrix:\\n\", cm_txt_only)\n",
    "\n",
    "X_train_bow_only = hstack([X_train_img, X_train_txt])\n",
    "X_test_bow_only  = hstack([X_test_img, X_test_txt])\n",
    "\n",
    "# --- Run Logistic Regression ---\n",
    "# Using max_iter=5000 for stability with large sparse feature matrices\n",
    "logreg_bow_only = LogisticRegression(max_iter=5000)\n",
    "logreg_bow_only.fit(X_train_bow_only, y_train)\n",
    "\n",
    "# --- Predict and Evaluate ---\n",
    "y_pred_bow_only = logreg_bow_only.predict(X_test_bow_only)\n",
    "acc_bow_only = accuracy_score(y_test, y_pred_bow_only)\n",
    "cm_bow_only = confusion_matrix(y_test, y_pred_bow_only)\n",
    "\n",
    "# --- Print Results ---\n",
    "print(\"\\n--- Model: Image Labels + Description Text ONLY ---\")\n",
    "print(\"Accuracy:\", round(acc_bow_only, 4))\n",
    "print(\"Confusion matrix:\\n\", cm_bow_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqjKGU-LZizt"
   },
   "source": [
    "The accuracy for each of the first three logistic regressions we did was almost identical. We added duration in as a factor for each of the first three models for uniformity. But, due to the almost identical accuracies we believed that the duration of the business fundraiser held too much weight. To verify our assumption we performed logistic regression again on each model but this time without factoring in the duration of the fundraiser.\n",
    "\n",
    "All three new models (without duration) performed slightly worse, with the description text model only obtaining an accuracy of 69.5%. And, the image labels + description text model getting the lowest accuracy of 69%. This proved that duration was the factor in our model that was leading us to get such similar accuracy scores. In the image label only model the accuracy was still quite close to the duration models, but when paired with description text the accuracy dropped. We believe this means description text is the worst at predicting how much a fundraiser will make, and image labels do a slightly better job.\n",
    "\n",
    "Overall, the duration of a fundraiser does the best at predicting how much each fundraiser will make. We got our best prediction scores when we used duration with image labels, and all three factors together. In order to make the most money for your fundraiser, you most likely need to have it running for a longer period of time. Images also are more likely to inspire donations from your audience than your description of your fundraiser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYRJ41jDeaof"
   },
   "source": [
    "Task E: Perform topic modeling (LDA) on the original image_labels or word descriptions depending on which category gave higher accuracy in D. If the combination of image_labels and description words gave the highest accuracy, use them together. Choose an appropriate number of topics. You may want to start with 4-5 topics, but adjust the number up or down depending on the word distributions you get. Decide on suitable names for each topic. Now sort the data from high to low \\$ raised (donâ€™t use the binary column, use the actual \\$), and consider the highest and the lowest quartiles of $. What are the main differences in the average topic weights of images across the two quartiles (e.g., greater weight of some topics in the highest versus lowest quartiles)? Show the main results in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DalBHkKNeafd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 'TaskE_word_topic_matrix.csv' (wordâ€“topic probabilities)\n",
      "\n",
      "âœ… Saved 'TaskF_topic_quartile_comparison.csv'\n",
      "         High $ (Top 25%)  Low $ (Bottom 25%)  Difference\n",
      "Topic_3             0.368               0.064       0.304\n",
      "Topic_2             0.369               0.119       0.250\n",
      "Topic_5             0.179               0.076       0.103\n",
      "Topic_1             0.030               0.035      -0.005\n",
      "Topic_4             0.053               0.706      -0.653\n"
     ]
    }
   ],
   "source": [
    "# --- Task E: Topic Modeling + Quartile Comparison ---\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# 1ï¸âƒ£ Combine text fields (if both labels and descriptions exist)\n",
    "df_labeled['labels'] = df_labeled['labels'].fillna(\"\")\n",
    "df_labeled['description'] = df_labeled['description'].fillna(\"\")\n",
    "df_labeled['combined_text'] = df_labeled['labels'] + \" \" + df_labeled['description']\n",
    "\n",
    "# 2ï¸âƒ£ Bag-of-Words representation\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(df_labeled['combined_text'])\n",
    "words = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "# 3ï¸âƒ£ Fit LDA model (5 topics)\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "topic_matrix = lda.fit_transform(X)\n",
    "\n",
    "# 4ï¸âƒ£ Save wordâ€“topic probabilities (for naming topics)\n",
    "topic_word_matrix = lda.components_\n",
    "topic_word_probs = topic_word_matrix / topic_word_matrix.sum(axis=1)[:, np.newaxis]\n",
    "word_topic_matrix = pd.DataFrame(topic_word_probs.T,\n",
    "                                 columns=[f\"Topic_{i+1}\" for i in range(lda.n_components)])\n",
    "word_topic_matrix.insert(0, \"word\", words)\n",
    "word_topic_matrix.to_csv(\"TaskE_word_topic_matrix3.csv\", index=False)\n",
    "print(\"âœ… Saved 'TaskE_word_topic_matrix.csv' (wordâ€“topic probabilities)\")\n",
    "\n",
    "# 5ï¸âƒ£ Quartile comparison on amount_raised\n",
    "topic_cols = [f\"Topic_{i+1}\" for i in range(lda.n_components)]\n",
    "df_topics = pd.concat([df_labeled[['amount_raised']].reset_index(drop=True),\n",
    "                       pd.DataFrame(topic_matrix, columns=topic_cols)], axis=1)\n",
    "\n",
    "# Normalize topic weights per campaign\n",
    "df_topics[topic_cols] = df_topics[topic_cols].div(df_topics[topic_cols].sum(axis=1), axis=0)\n",
    "\n",
    "# Split into quartiles by actual $ raised\n",
    "df_sorted = df_topics.sort_values('amount_raised', ascending=False)\n",
    "q1 = df_sorted['amount_raised'].quantile(0.25)\n",
    "q3 = df_sorted['amount_raised'].quantile(0.75)\n",
    "top_quartile = df_sorted[df_sorted['amount_raised'] >= q3]\n",
    "bottom_quartile = df_sorted[df_sorted['amount_raised'] <= q1]\n",
    "\n",
    "# Calculate average topic weights and differences\n",
    "top_avg = top_quartile[topic_cols].mean()\n",
    "bottom_avg = bottom_quartile[topic_cols].mean()\n",
    "topic_comparison = pd.DataFrame({\n",
    "    'High $ (Top 25%)': top_avg,\n",
    "    'Low $ (Bottom 25%)': bottom_avg\n",
    "})\n",
    "topic_comparison['Difference'] = topic_comparison['High $ (Top 25%)'] - topic_comparison['Low $ (Bottom 25%)']\n",
    "topic_comparison = topic_comparison.sort_values('Difference', ascending=False)\n",
    "\n",
    "# Save only the comparison table\n",
    "topic_comparison.to_csv(\"TaskF_topic_quartile_comparison3.csv\")\n",
    "print(\"\\nâœ… Saved 'TaskF_topic_quartile_comparison.csv'\")\n",
    "print(topic_comparison.round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4AkZZHQeaV4"
   },
   "source": [
    "Task F: What advice would you give to the organization if it wants to increase the $ raised based on your findings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38zvBdokeZ2L"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Average Topic Weights by Quartile:\n",
      "         High $ (Top 25%)  Low $ (Bottom 25%)  Difference\n",
      "Topic_1             0.156               0.196      -0.039\n",
      "Topic_2             0.262               0.217       0.045\n",
      "Topic_3             0.300               0.213       0.087\n",
      "Topic_4             0.152               0.195      -0.043\n",
      "Topic_5             0.130               0.179      -0.049\n",
      "\n",
      "ðŸ”¹ Topic 1 top words: ['retail', 'text', 'finger', 'design', 'shelf', 'customer', 'furniture', 'restaurant', 'beauty', 'food']\n",
      "\n",
      "ðŸ”¹ Topic 2 top words: ['red', 'wood', 'art', 'symbol', 'advertising', 'font', 'logo', 'graphic', 'graphics', 'design']\n",
      "\n",
      "ðŸ”¹ Topic 3 top words: ['chin', 'glasses', 'eyewear', 'care', 'hair', 'eyebrow', 'expression', 'facial', 'happiness', 'smile']\n",
      "\n",
      "ðŸ”¹ Topic 4 top words: ['arts', 'animation', 'car', 'cartoon', 'carnivores', 'motor', 'tire', 'automotive', 'blue', 'vehicle']\n",
      "\n",
      "ðŸ”¹ Topic 5 top words: ['brake', 'aircraft', 'electronic', 'camera', 'hat', 'cap', 'night', 'food', 'light', 'automotive']\n"
     ]
    }
   ],
   "source": [
    "# --- Task E: Topic Modeling (LDA) on Image Labels ---\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1ï¸âƒ£ Vectorize the image labels\n",
    "# (We reuse df_labeled['labels'] from previous tasks)\n",
    "vectorizer_lda = CountVectorizer(stop_words='english')\n",
    "X_lda = vectorizer_lda.fit_transform(df_labeled['labels'].fillna(\"\"))\n",
    "\n",
    "# 2ï¸âƒ£ Run LDA (start with 5 topics â€” adjust if needed)\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "topic_matrix = lda.fit_transform(X_lda)   # Each row = topic distribution for one campaign\n",
    "\n",
    "# 3ï¸âƒ£ Add topic weights to the dataframe\n",
    "topic_cols = [f\"Topic_{i+1}\" for i in range(lda.n_components)]\n",
    "df_topics = pd.concat([df_labeled.reset_index(drop=True),\n",
    "                       pd.DataFrame(topic_matrix, columns=topic_cols)], axis=1)\n",
    "\n",
    "# 4ï¸âƒ£ Sort by amount raised and split into top/bottom quartiles\n",
    "df_sorted = df_topics.sort_values('amount_raised', ascending=False)\n",
    "q1 = df_sorted['amount_raised'].quantile(0.25)\n",
    "q3 = df_sorted['amount_raised'].quantile(0.75)\n",
    "\n",
    "top_quartile = df_sorted[df_sorted['amount_raised'] >= q3]\n",
    "bottom_quartile = df_sorted[df_sorted['amount_raised'] <= q1]\n",
    "\n",
    "# 5ï¸âƒ£ Compute average topic weights for each quartile\n",
    "top_avg = top_quartile[topic_cols].mean()\n",
    "bottom_avg = bottom_quartile[topic_cols].mean()\n",
    "\n",
    "# 6ï¸âƒ£ Create a comparison table\n",
    "topic_comparison = pd.DataFrame({\n",
    "    'High $ (Top 25%)': top_avg,\n",
    "    'Low $ (Bottom 25%)': bottom_avg\n",
    "})\n",
    "topic_comparison['Difference'] = topic_comparison['High $ (Top 25%)'] - topic_comparison['Low $ (Bottom 25%)']\n",
    "\n",
    "print(\"\\nðŸ“Š Average Topic Weights by Quartile:\")\n",
    "print(topic_comparison.round(3))\n",
    "\n",
    "# 7ï¸âƒ£ Display top keywords for each topic\n",
    "words = np.array(vectorizer_lda.get_feature_names_out())\n",
    "for i, topic in enumerate(lda.components_):\n",
    "    top_words = [words[j] for j in topic.argsort()[-10:]]\n",
    "    print(f\"\\nðŸ”¹ Topic {i+1} top words: {top_words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['absorber', 'academic', 'active', 'advertising', 'aerospace',\n",
       "       'afro', 'agriculture', 'air', 'aircraft', 'album', 'alcoholic',\n",
       "       'aluminium', 'american', 'animal', 'animated', 'animation',\n",
       "       'appliance', 'apron', 'area', 'arm', 'art', 'artist', 'arts',\n",
       "       'asphalt', 'audio', 'automotive', 'aviation', 'awning', 'badge',\n",
       "       'bag', 'bakery', 'baking', 'balance', 'ball', 'ballet', 'balloon',\n",
       "       'bank', 'banner', 'bar', 'barefoot', 'barrel', 'barware',\n",
       "       'baseball', 'basket', 'beard', 'beauty', 'bed', 'bedrock', 'bench',\n",
       "       'berry', 'bicycle', 'billboard', 'bird', 'black', 'blond',\n",
       "       'blossom', 'blue', 'boat', 'body', 'bone', 'book', 'bookcase',\n",
       "       'bottle', 'bowl', 'brake', 'brambles', 'branch', 'bread',\n",
       "       'brewery', 'brick', 'brickwork', 'bridle', 'brown', 'building',\n",
       "       'bumper', 'buzz', 'cabin', 'cable', 'calf', 'california', 'camera',\n",
       "       'cameras', 'candle', 'canidae', 'cannoli', 'cap', 'caption', 'car',\n",
       "       'caravan', 'care', 'carnivores', 'cartilaginous', 'cartoon',\n",
       "       'case', 'cat', 'ceiling', 'chair', 'channel', 'character', 'cheek',\n",
       "       'cheese', 'chef', 'chicken', 'child', 'chin', 'chordophone',\n",
       "       'choreography', 'cigarette', 'classic', 'cleanliness', 'clip',\n",
       "       'clothing', 'coach', 'coffee', 'collage', 'collar', 'collision',\n",
       "       'comb', 'comfort', 'commercial', 'community', 'composite',\n",
       "       'contact', 'cook', 'cooking', 'corporate', 'cosmetics', 'costume',\n",
       "       'couch', 'countertop', 'cover', 'creative', 'crew', 'cricket',\n",
       "       'crossfit', 'crown', 'cuisine', 'cup', 'curl', 'customer', 'cut',\n",
       "       'cylinder', 'damper', 'dance', 'dancer', 'darkness', 'day',\n",
       "       'daytime', 'decal', 'denim', 'department', 'design', 'desk',\n",
       "       'dessert', 'device', 'digital', 'dishware', 'display', 'dog',\n",
       "       'domestic', 'door', 'drawing', 'dreadlocks', 'dress', 'drink',\n",
       "       'drinking', 'drinkware', 'driving', 'dusk', 'earring', 'education',\n",
       "       'elbow', 'electrical', 'electricity', 'electronic', 'emblem',\n",
       "       'emergency', 'enforcement', 'engineering', 'english',\n",
       "       'entertainment', 'equipment', 'establishment', 'event', 'exercise',\n",
       "       'expression', 'exterior', 'eye', 'eyebrow', 'eyelash', 'eyewear',\n",
       "       'facade', 'face', 'facial', 'family', 'fashion', 'fast', 'felidae',\n",
       "       'felinae', 'fence', 'fictional', 'film', 'fin', 'finger', 'fish',\n",
       "       'fishes', 'fitness', 'fixed', 'fixture', 'flag', 'flame', 'flat',\n",
       "       'floor', 'flooring', 'floral', 'floristry', 'flower', 'flowerpot',\n",
       "       'font', 'food', 'foods', 'foot', 'football', 'footwear',\n",
       "       'forehead', 'formal', 'fowl', 'frame', 'friendship', 'fuel', 'fun',\n",
       "       'furniture', 'gadget', 'game', 'garden', 'garment', 'geological',\n",
       "       'glass', 'glasses', 'goggles', 'government', 'graffiti', 'graphic',\n",
       "       'graphics', 'grass', 'greenhouse', 'group', 'guitar', 'guitarist',\n",
       "       'gym', 'hair', 'hairstyle', 'hand', 'handrail', 'handwriting',\n",
       "       'happiness', 'hardwood', 'hat', 'head', 'headgear', 'headphones',\n",
       "       'headpiece', 'headquarters', 'health', 'heart', 'herb', 'high',\n",
       "       'holder', 'horse', 'hospital', 'houseplant', 'human',\n",
       "       'illustration', 'ingredient', 'inn', 'institution', 'instrument',\n",
       "       'interior', 'iron', 'italian', 'jacket', 'jaw', 'jean', 'jeans',\n",
       "       'jersey', 'jewellery', 'jewelry', 'jheri', 'keg', 'kettlebell',\n",
       "       'kindergarten', 'kit', 'kitchen', 'knee', 'kneeling', 'knit',\n",
       "       'label', 'lake', 'land', 'lane', 'laughter', 'law', 'leaf',\n",
       "       'leisure', 'light', 'lighting', 'line', 'linens', 'lips', 'log',\n",
       "       'logo', 'long', 'lotus', 'love', 'luxury', 'machine', 'makeover',\n",
       "       'manufacturer', 'marsupial', 'mat', 'material', 'meat', 'medical',\n",
       "       'menu', 'metal', 'midnight', 'military', 'milkshake', 'mirror',\n",
       "       'mobile', 'mode', 'model', 'modern', 'molding', 'monochrome',\n",
       "       'motel', 'motif', 'motor', 'motorcycle', 'motorcycling',\n",
       "       'moustache', 'mouth', 'mural', 'muscle', 'musical', 'mustelids',\n",
       "       'nail', 'natural', 'nature', 'neck', 'neighbourhood', 'neon',\n",
       "       'news', 'night', 'nose', 'operator', 'optics', 'orange',\n",
       "       'organization', 'ornament', 'outdoor', 'outerwear', 'overhead',\n",
       "       'paint', 'painting', 'palm', 'pants', 'paper', 'parking', 'party',\n",
       "       'pastry', 'patient', 'people', 'performance', 'performing',\n",
       "       'personal', 'petal', 'phasianidae', 'phenomenon', 'phone', 'photo',\n",
       "       'photograph', 'photography', 'physical', 'picnic', 'picture',\n",
       "       'piece', 'pig', 'pink', 'pixie', 'pizza', 'plank', 'plantation',\n",
       "       'plastic', 'pleased', 'plywood', 'polo', 'pool', 'portrait',\n",
       "       'poster', 'powdered', 'power', 'produce', 'product', 'products',\n",
       "       'professional', 'pub', 'publication', 'purple', 'racket',\n",
       "       'racketlon', 'racquet', 'rapper', 'rear', 'recreation',\n",
       "       'recreational', 'red', 'reflection', 'reflex', 'reservoir',\n",
       "       'resort', 'restaurant', 'retail', 'riding', 'road', 'room',\n",
       "       'rubble', 'rust', 'sail', 'sailboat', 'sailing', 'salon',\n",
       "       'scholar', 'school', 'science', 'scrap', 'screenshot', 'secondary',\n",
       "       'selfie', 'serveware', 'service', 'shadow', 'shake', 'shark',\n",
       "       'shed', 'shelf', 'shelving', 'shirt', 'shock', 'shoot',\n",
       "       'shopkeeper', 'shorts', 'shoulder', 'siding', 'sign', 'signage',\n",
       "       'signature', 'silver', 'sitting', 'skateboard', 'skateboarding',\n",
       "       'skeleton', 'sketch', 'skin', 'skull', 'sleeve', 'sleeveless',\n",
       "       'smile', 'smoke', 'smoking', 'sneakers', 'snout', 'soccer',\n",
       "       'social', 'soft', 'software', 'soil', 'spa', 'spokesperson',\n",
       "       'sport', 'sports', 'spring', 'stain', 'standing', 'states',\n",
       "       'steering', 'stemware', 'sticker', 'straw', 'street', 'string',\n",
       "       'student', 'style', 'sugar', 'suidae', 'sun', 'sunglasses',\n",
       "       'super', 'supplies', 'supply', 'surface', 'swimming', 'symbol',\n",
       "       'symmetry', 'table', 'tableware', 'tail', 'tank', 'tattoo',\n",
       "       'teacup', 'team', 'telephony', 'temple', 'tennis', 'terrestrial',\n",
       "       'text', 'textile', 'thigh', 'thumb', 'tight', 'tire', 'tobacco',\n",
       "       'toe', 'tool', 'tooth', 'town', 'toy', 'tradesman', 'traffic',\n",
       "       'training', 'transparency', 'transport', 'travel', 'trees',\n",
       "       'triangle', 'truck', 'trucker', 'undergarment', 'underpants',\n",
       "       'uniform', 'united', 'urban', 'utility', 'vacation', 'van',\n",
       "       'varnish', 'vegetable', 'vehicle', 'venue', 'vertebrate',\n",
       "       'veterans', 'video', 'view', 'visibility', 'vision', 'visual',\n",
       "       'waist', 'wall', 'waste', 'watercraft', 'wax', 'wear', 'weights',\n",
       "       'wheel', 'whiskers', 'white', 'wicker', 'wig', 'window', 'wing',\n",
       "       'winter', 'wire', 'wood', 'woodland', 'wool', 'woolen', 'worker',\n",
       "       'working', 'workwear', 'wrist', 'yellow', 'yoga', 'youth'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Average Topic Weights by Quartile:\n",
      "         High $ (Top 25%)  Low $ (Bottom 25%)  Difference\n",
      "Topic_1             0.156               0.196      -0.039\n",
      "Topic_2             0.262               0.217       0.045\n",
      "Topic_3             0.300               0.213       0.087\n",
      "Topic_4             0.152               0.195      -0.043\n",
      "Topic_5             0.130               0.179      -0.049\n",
      "\n",
      "ðŸ”¹ Topic 1 top words:\n",
      "['chair', 'handwriting', 'interior', 'facial', 'hairstyle', 'retail', 'text', 'finger', 'design', 'shelf', 'customer', 'furniture', 'restaurant', 'beauty', 'food']\n",
      "\n",
      "ðŸ”¹ Topic 2 top words:\n",
      "['flooring', 'label', 'clip', 'pink', 'sign', 'red', 'wood', 'art', 'symbol', 'advertising', 'font', 'logo', 'graphic', 'graphics', 'design']\n",
      "\n",
      "ðŸ”¹ Topic 3 top words:\n",
      "['forehead', 'sleeve', 'lips', 'shirt', 'vision', 'chin', 'glasses', 'eyewear', 'care', 'hair', 'eyebrow', 'expression', 'facial', 'happiness', 'smile']\n",
      "\n",
      "ðŸ”¹ Topic 4 top words:\n",
      "['fictional', 'transport', 'mode', 'exterior', 'dog', 'arts', 'animation', 'car', 'cartoon', 'carnivores', 'motor', 'tire', 'automotive', 'blue', 'vehicle']\n",
      "\n",
      "ðŸ”¹ Topic 5 top words:\n",
      "['lighting', 'parking', 'sunglasses', 'device', 'tail', 'brake', 'aircraft', 'electronic', 'camera', 'hat', 'cap', 'night', 'food', 'light', 'automotive']\n",
      "\n",
      "âœ… Topic words saved to 'lda_topic_words.csv'\n"
     ]
    }
   ],
   "source": [
    "# --- Task E: Topic Modeling (LDA) on Image Labels and Export Topic Words ---\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1ï¸âƒ£ Vectorize the image labels\n",
    "vectorizer_lda = CountVectorizer(stop_words='english')\n",
    "X_lda = vectorizer_lda.fit_transform(df_labeled['labels'].fillna(\"\"))\n",
    "\n",
    "# 2ï¸âƒ£ Run LDA (you can adjust number of topics)\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "topic_matrix = lda.fit_transform(X_lda)\n",
    "\n",
    "# 3ï¸âƒ£ Add topic weights to the dataframe\n",
    "topic_cols = [f\"Topic_{i+1}\" for i in range(lda.n_components)]\n",
    "df_topics = pd.concat([df_labeled.reset_index(drop=True),\n",
    "                       pd.DataFrame(topic_matrix, columns=topic_cols)], axis=1)\n",
    "\n",
    "# 4ï¸âƒ£ Sort by amount raised and split into top/bottom quartiles\n",
    "df_sorted = df_topics.sort_values('amount_raised', ascending=False)\n",
    "q1 = df_sorted['amount_raised'].quantile(0.25)\n",
    "q3 = df_sorted['amount_raised'].quantile(0.75)\n",
    "top_quartile = df_sorted[df_sorted['amount_raised'] >= q3]\n",
    "bottom_quartile = df_sorted[df_sorted['amount_raised'] <= q1]\n",
    "\n",
    "# 5ï¸âƒ£ Compute average topic weights for each quartile\n",
    "top_avg = top_quartile[topic_cols].mean()\n",
    "bottom_avg = bottom_quartile[topic_cols].mean()\n",
    "\n",
    "# 6ï¸âƒ£ Create comparison table\n",
    "topic_comparison = pd.DataFrame({\n",
    "    'High $ (Top 25%)': top_avg,\n",
    "    'Low $ (Bottom 25%)': bottom_avg\n",
    "})\n",
    "topic_comparison['Difference'] = topic_comparison['High $ (Top 25%)'] - topic_comparison['Low $ (Bottom 25%)']\n",
    "\n",
    "print(\"\\nðŸ“Š Average Topic Weights by Quartile:\")\n",
    "print(topic_comparison.round(3))\n",
    "\n",
    "# 7ï¸âƒ£ Extract top keywords for each topic and save to CSV\n",
    "words = np.array(vectorizer_lda.get_feature_names_out())\n",
    "topic_words = []\n",
    "\n",
    "for i, topic in enumerate(lda.components_):\n",
    "    top_word_list = [words[j] for j in topic.argsort()[-15:]]  # Top 15 words per topic\n",
    "    topic_words.append({\n",
    "        \"Topic\": f\"Topic_{i+1}\",\n",
    "        \"Top_Words\": \", \".join(top_word_list)\n",
    "    })\n",
    "    print(f\"\\nðŸ”¹ Topic {i+1} top words:\")\n",
    "    print(top_word_list)\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "df_topic_words = pd.DataFrame(topic_words)\n",
    "df_topic_words.to_csv(\"lda_topic_words.csv\", index=False)\n",
    "\n",
    "print(\"\\nâœ… Topic words saved to 'lda_topic_words.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Top_Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Topic_1</td>\n",
       "      <td>chair, handwriting, interior, facial, hairstyl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Topic_2</td>\n",
       "      <td>flooring, label, clip, pink, sign, red, wood, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Topic_3</td>\n",
       "      <td>forehead, sleeve, lips, shirt, vision, chin, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Topic_4</td>\n",
       "      <td>fictional, transport, mode, exterior, dog, art...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Topic_5</td>\n",
       "      <td>lighting, parking, sunglasses, device, tail, b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic                                          Top_Words\n",
       "0  Topic_1  chair, handwriting, interior, facial, hairstyl...\n",
       "1  Topic_2  flooring, label, clip, pink, sign, red, wood, ...\n",
       "2  Topic_3  forehead, sleeve, lips, shirt, vision, chin, g...\n",
       "3  Topic_4  fictional, transport, mode, exterior, dog, art...\n",
       "4  Topic_5  lighting, parking, sunglasses, device, tail, b..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Topic 1 top words:\n",
      "['electrical', 'cable', 'power', 'electricity', 'overhead', 'camera', 'cameras', 'optics', 'aviation', 'fixed', 'aerospace', 'brake', 'tail', 'digital', 'reflex', 'manufacturer', 'military', 'travel', 'air', 'engineering', 'operator', 'film', 'land', 'trucker', 'cricket', 'motor', 'mirror', 'driving', 'steering', 'rear', 'view', 'pizza', 'italian', 'cuisine', 'sun', 'leaf', 'secondary', 'institution', 'baseball', 'van', 'bumper', 'establishment', 'vegetable', 'rapper', 'window', 'town', 'facade', 'neighbourhood', 'area', 'urban', 'greenhouse', 'agriculture', 'plantation', 'woolen', 'wool', 'knit', 'branch', 'spring', 'grass', 'shoot', 'cosmetics', 'shadow', 'wig', 'earring', 'jewelry', 'barware', 'mode', 'transport', 'varnish', 'exterior', 'uniform', 'graffiti', 'tradesman', 'tool', 'soil', 'tire', 'chicken', 'comb', 'fowl', 'phasianidae', 'bird', 'goggles', 'chef', 'dishware', 'science', 'education', 'drink', 'stemware', 'alcoholic', 'spokesperson', 'government', 'street', 'toy', 'caravan', 'recreational', 'mural', 'tennis', 'racket', 'linens', 'silver', 'kitchen', 'appliance', 'telephony', 'animated', 'felidae', 'felinae', 'cat', 'cartoon', 'bench', 'outdoor', 'whiskers', 'luxury', 'vertebrate', 'racketlon', 'racquet', 'soft', 'veterans', 'mustelids', 'marsupial', 'yoga', 'natural', 'canidae', 'woodland', 'lake', 'reservoir', 'reflection', 'bank', 'channel', 'meat', 'suidae', 'domestic', 'pig', 'baking', 'damper', 'bread', 'medical', 'table', 'picnic', 'foods', 'automotive', 'illustration', 'plank', 'hospital', 'visual', 'patient', 'bed', 'performing', 'parking', 'skateboarding', 'performance', 'jersey', 'youth', 'tight', 'snout', 'carnivores', 'bridle', 'horse', 'footwear', 'english', 'riding', 'bottle', 'vision', 'glasses', 'pants', 'mat', 'musical', 'wheel', 'instrument', 'hardwood', 'decal', 'emblem', 'dance', 'skateboard', 'knee', 'event', 'ball', 'soccer', 'log', 'cabin', 'brambles', 'berry', 'professional', 'coach', 'crossfit', 'blossom', 'swimming', 'resort', 'pool', 'signature', 'model', 'choreography', 'denim', 'palm', 'dusk', 'jean', 'calf', 'shorts', 'drawing', 'kit', 'candle', 'sport', 'paper', 'cheek', 'sneakers', 'cover', 'wire', 'face', 'jewellery', 'terrestrial', 'drinking', 'milkshake', 'straw', 'shake', 'dress', 'trees', 'flag', 'photograph', 'skin', 'guitarist', 'guitar', 'chordophone', 'string', 'ballet', 'dancer', 'holder', 'wax', 'scholar', 'boat', 'sailing', 'sailboat', 'sail', 'watercraft', 'classic', 'crown', 'headpiece', 'friendship', 'pleased', 'product', 'couch', 'bag', 'waste', 'barrel', 'brewery', 'keg', 'cylinder', 'flat', 'creative', 'cook', 'toe', 'barefoot', 'community', 'road', 'stain', 'clip', 'apron', 'iron', 'paint', 'worker', 'balance', 'eyewear', 'crew', 'skeleton', 'bone', 'skull', 'metal', 'dog', 'united', 'states', 'care', 'fashion', 'long', 'car', 'transparency', 'headgear', 'organization', 'kneeling', 'shark', 'cartilaginous', 'fin', 'fishes', 'flower', 'floristry', 'buzz', 'surface', 'lane', 'caption', 'mobile', 'phone', 'collar', 'hat', 'contact', 'training', 'rust', 'scrap', 'siding', 'inn', 'motel', 'painting', 'modern', 'lotus', 'triangle', 'people', 'utility', 'animal', 'laughter', 'news', 'drinkware', 'wall', 'arts', 'bicycle', 'academic', 'fitness', 'nature', 'wear', 'formal', 'foot', 'outerwear', 'photo', 'physical', 'awning', 'day', 'school', 'polo', 'mouth', 'album', 'recreation', 'student', 'asphalt', 'winter', 'health', 'badge', 'textile', 'daytime', 'composite', 'eye', 'sunglasses', 'petal', 'child', 'door', 'label', 'garden', 'room', 'collage', 'makeover', 'cheese', 'style', 'molding', 'blond', 'standing', 'vacation', 'family', 'blue', 'fish', 'leisure', 'purple', 'glass', 'pink', 'jacket', 'california', 'jeans', 'working', 'billboard', 'brown', 'sticker', 'gym', 'fence', 'handrail', 'basket', 'wicker', 'kindergarten', 'temple', 'yellow', 'personal', 'sleeveless', 'shed', 'red', 'chin', 'banner', 'device', 'graphics', 'electronic', 'symbol', 'wood', 'active', 'sign', 'forehead', 'smile', 'aircraft', 'light', 'graphic', 'head', 'signage', 'selfie', 'fun', 'gadget', 'shirt', 'eyebrow', 'font', 'screenshot', 'fictional', 'character', 'sleeve', 'nose', 'advertising', 'publication', 'entertainment', 'cut', 'wing', 'vehicle', 'white', 'monochrome', 'night', 'jaw', 'plastic', 'truck', 'cap', 'team', 'floor', 'book', 'heart', 'comfort', 'fixture', 'midnight', 'flooring', 'commercial', 'collision', 'traffic', 'display', 'desk', 'audio', 'produce', 'bookcase', 'arm', 'case', 'pub', 'bar', 'tableware', 'artist', 'tattoo', 'cleanliness', 'plywood', 'elbow', 'poster', 'headphones', 'dreadlocks', 'art', 'video', 'portrait', 'pixie', 'game', 'software', 'social', 'motif', 'ornament', 'symmetry', 'salon', 'cooking', 'brick', 'department', 'material', 'enforcement', 'law', 'supply', 'serveware', 'spa', 'body', 'brickwork', 'corporate', 'sketch', 'herb', 'powdered', 'sugar', 'kettlebell', 'human', 'underpants', 'undergarment', 'teacup', 'bakery', 'pastry', 'cannoli', 'frame', 'picture', 'rubble', 'bedrock', 'smoking', 'cigarette', 'products', 'motorcycle', 'tank', 'fuel', 'motorcycling', 'absorber', 'shock', 'balloon', 'garment', 'piece', 'menu', 'american', 'super', 'bowl', 'line', 'aluminium', 'thigh', 'sports', 'supplies', 'venue', 'love', 'sitting', 'waist', 'flowerpot', 'geological', 'phenomenon', 'flame', 'weights', 'orange', 'animation', 'lighting', 'machine', 'building', 'photography', 'darkness', 'eyelash', 'group', 'floral', 'nail', 'visibility', 'workwear', 'curl', 'jheri', 'houseplant', 'headquarters', 'countertop', 'shopkeeper', 'tobacco', 'party', 'high', 'costume', 'exercise', 'football', 'coffee', 'cup', 'equipment', 'expression', 'ceiling', 'neon', 'afro', 'muscle', 'service', 'wrist', 'smoke', 'thumb', 'logo', 'black', 'clothing', 'lips', 'beard', 'tooth', 'hand', 'shoulder', 'happiness', 'moustache', 'shelving', 'dessert', 'fast', 'ingredient', 'hair', 'neck', 'emergency', 'chair', 'handwriting', 'interior', 'facial', 'hairstyle', 'retail', 'text', 'finger', 'design', 'shelf', 'customer', 'furniture', 'restaurant', 'beauty', 'food']\n",
      "\n",
      "ðŸ”¹ Topic 2 top words:\n",
      "['electrical', 'electricity', 'power', 'cable', 'overhead', 'camera', 'optics', 'cameras', 'aviation', 'fixed', 'aerospace', 'digital', 'reflex', 'military', 'manufacturer', 'travel', 'engineering', 'air', 'operator', 'film', 'land', 'cricket', 'trucker', 'cuisine', 'italian', 'rear', 'mirror', 'driving', 'steering', 'view', 'sun', 'establishment', 'american', 'bowl', 'super', 'leaf', 'party', 'tobacco', 'vegetable', 'baseball', 'institution', 'secondary', 'shoot', 'rapper', 'urban', 'area', 'facade', 'neighbourhood', 'window', 'town', 'greenhouse', 'plantation', 'agriculture', 'knit', 'woolen', 'wool', 'branch', 'grass', 'spring', 'piece', 'balloon', 'garment', 'products', 'cigarette', 'smoking', 'jewelry', 'cosmetics', 'shadow', 'earring', 'wig', 'curl', 'jheri', 'varnish', 'barware', 'soil', 'tradesman', 'tool', 'uniform', 'rubble', 'bedrock', 'shopkeeper', 'menu', 'frame', 'picture', 'cannoli', 'pastry', 'bakery', 'sugar', 'powdered', 'kettlebell', 'phasianidae', 'comb', 'fowl', 'chicken', 'bird', 'dishware', 'chef', 'human', 'underpants', 'undergarment', 'damper', 'bread', 'baking', 'teacup', 'gadget', 'department', 'goggles', 'toy', 'education', 'science', 'silver', 'linens', 'appliance', 'kitchen', 'telephony', 'caravan', 'recreational', 'countertop', 'fuel', 'absorber', 'shock', 'tank', 'motorcycling', 'motorcycle', 'felidae', 'felinae', 'cat', 'thumb', 'alcoholic', 'stemware', 'drink', 'cap', 'government', 'spokesperson', 'whiskers', 'herb', 'workwear', 'visibility', 'device', 'bumper', 'yoga', 'supply', 'channel', 'lake', 'reflection', 'reservoir', 'bank', 'woodland', 'medical', 'tight', 'headquarters', 'hospital', 'law', 'enforcement', 'bed', 'patient', 'performance', 'fixture', 'houseplant', 'jersey', 'youth', 'publication', 'sketch', 'midnight', 'vision', 'musical', 'glasses', 'pants', 'mat', 'instrument', 'corporate', 'book', 'model', 'event', 'monochrome', 'cut', 'lighting', 'choreography', 'chin', 'cheek', 'sneakers', 'jewellery', 'face', 'paper', 'wrist', 'muscle', 'eyebrow', 'dress', 'body', 'worker', 'skin', 'forehead', 'photograph', 'jaw', 'friendship', 'pleased', 'scholar', 'string', 'guitarist', 'guitar', 'chordophone', 'audio', 'serveware', 'head', 'cook', 'apron', 'product', 'community', 'road', 'sleeve', 'candle', 'crew', 'balance', 'eyewear', 'metal', 'hairstyle', 'iron', 'waste', 'bag', 'long', 'mobile', 'phone', 'wire', 'display', 'headgear', 'afro', 'buzz', 'kneeling', 'organization', 'outerwear', 'collar', 'neck', 'surface', 'lane', 'transparency', 'wax', 'holder', 'hat', 'moustache', 'high', 'bookcase', 'cartilaginous', 'fin', 'fishes', 'shark', 'lips', 'case', 'exercise', 'people', 'academic', 'smoke', 'shoulder', 'formal', 'wear', 'mouth', 'nature', 'cooking', 'school', 'pixie', 'portrait', 'student', 'floristry', 'flower', 'asphalt', 'textile', 'daytime', 'clothing', 'social', 'eye', 'team', 'blond', 'arm', 'flowerpot', 'selfie', 'brickwork', 'waist', 'garden', 'eyelash', 'photography', 'molding', 'aluminium', 'standing', 'elbow', 'nose', 'weights', 'jacket', 'headphones', 'tooth', 'motor', 'brown', 'kindergarten', 'temple', 'dreadlocks', 'spa', 'exterior', 'mode', 'transport', 'hair', 'sleeveless', 'van', 'beauty', 'sunglasses', 'finger', 'fashion', 'performing', 'entertainment', 'cartoon', 'dessert', 'shelf', 'graffiti', 'ingredient', 'facial', 'electronic', 'animated', 'comfort', 'tail', 'brake', 'light', 'expression', 'parking', 'smile', 'street', 'aircraft', 'luxury', 'dance', 'shelving', 'illustration', 'emergency', 'fast', 'vertebrate', 'door', 'winter', 'mural', 'phenomenon', 'geological', 'truck', 'suidae', 'pig', 'domestic', 'meat', 'carnivores', 'food', 'costume', 'pizza', 'hand', 'dog', 'group', 'canidae', 'shirt', 'wing', 'signage', 'customer', 'chair', 'denim', 'dancer', 'ballet', 'service', 'cover', 'fictional', 'character', 'palm', 'happiness', 'trees', 'animation', 'drinking', 'polo', 'active', 'nail', 'visual', 'arts', 'vehicle', 'black', 'shake', 'milkshake', 'straw', 'health', 'snout', 'night', 'salon', 'creative', 'care', 'blue', 'child', 'handwriting', 'album', 'tire', 'floral', 'software', 'game', 'video', 'beard', 'ornament', 'symmetry', 'motif', 'flame', 'makeover', 'bone', 'skull', 'skeleton', 'purple', 'screenshot', 'jeans', 'yellow', 'orange', 'collage', 'neon', 'wicker', 'basket', 'personal', 'darkness', 'text', 'sticker', 'billboard', 'fence', 'handrail', 'wheel', 'automotive', 'working', 'family', 'california', 'cheese', 'style', 'utility', 'plywood', 'bar', 'tattoo', 'artist', 'pub', 'ceiling', 'equipment', 'modern', 'painting', 'shed', 'news', 'laughter', 'cup', 'coffee', 'wall', 'football', 'recreation', 'line', 'physical', 'thigh', 'vacation', 'foot', 'sitting', 'venue', 'supplies', 'states', 'united', 'petal', 'motel', 'siding', 'inn', 'triangle', 'lotus', 'rust', 'scrap', 'contact', 'training', 'classic', 'barefoot', 'toe', 'barrel', 'cylinder', 'keg', 'brewery', 'crown', 'headpiece', 'sail', 'watercraft', 'boat', 'sailboat', 'sailing', 'swimming', 'pool', 'resort', 'flat', 'couch', 'dusk', 'jean', 'calf', 'shorts', 'drawing', 'coach', 'crossfit', 'professional', 'blossom', 'berry', 'brambles', 'signature', 'cabin', 'log', 'knee', 'skateboard', 'soccer', 'ball', 'bridle', 'footwear', 'horse', 'riding', 'english', 'mustelids', 'marsupial', 'veterans', 'picnic', 'table', 'racquet', 'racketlon', 'soft', 'room', 'glass', 'fish', 'gym', 'fun', 'paint', 'retail', 'photo', 'love', 'cleanliness', 'tableware', 'badge', 'collision', 'traffic', 'caption', 'desk', 'terrestrial', 'decal', 'kit', 'fitness', 'emblem', 'bottle', 'plank', 'skateboarding', 'bench', 'outdoor', 'racket', 'tennis', 'drinkware', 'sports', 'brick', 'day', 'composite', 'restaurant', 'machine', 'commercial', 'bicycle', 'car', 'produce', 'interior', 'awning', 'sport', 'heart', 'hardwood', 'flag', 'foods', 'leisure', 'building', 'stain', 'animal', 'natural', 'plastic', 'material', 'furniture', 'white', 'poster', 'floor', 'banner', 'flooring', 'label', 'clip', 'pink', 'sign', 'red', 'wood', 'art', 'symbol', 'advertising', 'font', 'logo', 'graphic', 'graphics', 'design']\n",
      "\n",
      "ðŸ”¹ Topic 3 top words:\n",
      "['electrical', 'overhead', 'electricity', 'power', 'cable', 'camera', 'optics', 'cameras', 'aviation', 'fixed', 'aerospace', 'digital', 'reflex', 'military', 'manufacturer', 'air', 'travel', 'engineering', 'operator', 'film', 'land', 'super', 'bowl', 'american', 'italian', 'cuisine', 'trucker', 'cricket', 'motor', 'driving', 'steering', 'view', 'mirror', 'rear', 'establishment', 'aircraft', 'vegetable', 'van', 'leaf', 'bumper', 'secondary', 'institution', 'urban', 'neighbourhood', 'town', 'window', 'area', 'facade', 'varnish', 'rapper', 'agriculture', 'plantation', 'greenhouse', 'menu', 'tank', 'motorcycle', 'motorcycling', 'fuel', 'shock', 'absorber', 'frame', 'picture', 'dessert', 'earring', 'cosmetics', 'wig', 'shadow', 'jewelry', 'party', 'barware', 'shoot', 'wool', 'knit', 'woolen', 'exterior', 'branch', 'spring', 'grass', 'graffiti', 'tire', 'shopkeeper', 'countertop', 'cannoli', 'pastry', 'bakery', 'teacup', 'department', 'underpants', 'undergarment', 'human', 'sugar', 'powdered', 'sketch', 'sun', 'comb', 'phasianidae', 'bird', 'chicken', 'fowl', 'bread', 'damper', 'baking', 'dishware', 'chef', 'street', 'garment', 'balloon', 'piece', 'toy', 'education', 'science', 'stemware', 'alcoholic', 'drink', 'appliance', 'kitchen', 'caravan', 'recreational', 'mural', 'tobacco', 'racket', 'tennis', 'outdoor', 'bench', 'silver', 'linens', 'felidae', 'felinae', 'cat', 'bedrock', 'rubble', 'fixture', 'kettlebell', 'chair', 'whiskers', 'fictional', 'character', 'emergency', 'wing', 'vertebrate', 'goggles', 'visibility', 'workwear', 'products', 'smoking', 'cigarette', 'racketlon', 'soft', 'racquet', 'mustelids', 'marsupial', 'table', 'picnic', 'natural', 'veterans', 'canidae', 'thumb', 'telephony', 'woodland', 'channel', 'lake', 'bank', 'reservoir', 'reflection', 'suidae', 'meat', 'domestic', 'pig', 'shelving', 'herb', 'visual', 'foods', 'publication', 'plank', 'signage', 'headquarters', 'parking', 'skateboarding', 'bridle', 'riding', 'footwear', 'horse', 'english', 'ball', 'soccer', 'shelf', 'supply', 'bottle', 'nail', 'midnight', 'houseplant', 'book', 'wheel', 'service', 'lighting', 'paper', 'emblem', 'salon', 'corporate', 'fast', 'knee', 'skateboard', 'signature', 'brambles', 'berry', 'couch', 'cabin', 'log', 'drinking', 'shake', 'straw', 'milkshake', 'drawing', 'flat', 'floor', 'law', 'enforcement', 'dusk', 'calf', 'jean', 'shorts', 'blossom', 'sport', 'product', 'denim', 'floral', 'body', 'ingredient', 'flooring', 'coach', 'crossfit', 'professional', 'cover', 'software', 'game', 'football', 'case', 'symmetry', 'ornament', 'headpiece', 'crown', 'watercraft', 'sail', 'sailing', 'sailboat', 'boat', 'keg', 'cylinder', 'brewery', 'barrel', 'venue', 'classic', 'serveware', 'display', 'toe', 'barefoot', 'cook', 'heart', 'costume', 'paint', 'balance', 'skull', 'skeleton', 'bone', 'supplies', 'interior', 'painting', 'modern', 'desk', 'pool', 'swimming', 'resort', 'bookcase', 'video', 'drinkware', 'lotus', 'triangle', 'training', 'contact', 'rust', 'scrap', 'siding', 'inn', 'motel', 'cup', 'coffee', 'building', 'wall', 'utility', 'retail', 'truck', 'bicycle', 'awning', 'smoke', 'foot', 'motif', 'line', 'produce', 'badge', 'brickwork', 'sitting', 'album', 'traffic', 'collision', 'ceiling', 'thigh', 'artist', 'tattoo', 'restaurant', 'neon', 'machine', 'recreation', 'handwriting', 'plywood', 'tableware', 'elbow', 'darkness', 'pub', 'commercial', 'working', 'bar', 'billboard', 'brick', 'sticker', 'cleanliness', 'spa', 'food', 'symbol', 'sign', 'light', 'vehicle', 'automotive', 'night', 'graphic', 'tail', 'brake', 'snout', 'pizza', 'label', 'banner', 'animal', 'curl', 'jheri', 'uniform', 'decal', 'kit', 'animation', 'design', 'apron', 'finger', 'baseball', 'font', 'blue', 'terrestrial', 'customer', 'art', 'purple', 'clip', 'logo', 'palm', 'tradesman', 'soil', 'tool', 'illustration', 'graphics', 'brown', 'hardwood', 'trees', 'car', 'cap', 'youth', 'jersey', 'sports', 'carnivores', 'poster', 'flag', 'hand', 'government', 'spokesperson', 'united', 'states', 'red', 'cooking', 'animated', 'text', 'electronic', 'mode', 'transport', 'luxury', 'cartoon', 'screenshot', 'gadget', 'device', 'advertising', 'california', 'cheese', 'style', 'wrist', 'caption', 'model', 'photo', 'dog', 'group', 'comfort', 'muscle', 'personal', 'hairstyle', 'shed', 'laughter', 'news', 'neck', 'makeover', 'worker', 'furniture', 'afro', 'beauty', 'moustache', 'social', 'blond', 'gym', 'weights', 'pixie', 'portrait', 'formal', 'wear', 'daytime', 'eye', 'standing', 'headphones', 'eyelash', 'nose', 'jacket', 'temple', 'tooth', 'creative', 'stain', 'dreadlocks', 'photography', 'wood', 'fence', 'handrail', 'sunglasses', 'jeans', 'basket', 'wicker', 'collage', 'school', 'sleeveless', 'material', 'asphalt', 'lane', 'surface', 'geological', 'phenomenon', 'student', 'composite', 'high', 'day', 'nature', 'polo', 'flowerpot', 'waist', 'academic', 'dancer', 'ballet', 'phone', 'mobile', 'winter', 'fish', 'kindergarten', 'molding', 'room', 'outerwear', 'flower', 'floristry', 'organization', 'kneeling', 'shark', 'cartilaginous', 'fin', 'fishes', 'transparency', 'buzz', 'wax', 'holder', 'waste', 'bag', 'scholar', 'pleased', 'friendship', 'guitar', 'chordophone', 'guitarist', 'string', 'wire', 'sneakers', 'choreography', 'musical', 'mat', 'pants', 'tight', 'garden', 'clothing', 'leisure', 'love', 'door', 'yellow', 'arm', 'flame', 'road', 'mouth', 'plastic', 'textile', 'crew', 'aluminium', 'dance', 'vacation', 'glass', 'community', 'photograph', 'jewellery', 'candle', 'instrument', 'performance', 'patient', 'bed', 'petal', 'health', 'pink', 'long', 'family', 'team', 'audio', 'child', 'metal', 'iron', 'cut', 'selfie', 'event', 'hospital', 'medical', 'yoga', 'fitness', 'physical', 'white', 'exercise', 'monochrome', 'entertainment', 'performing', 'arts', 'hat', 'shoulder', 'dress', 'collar', 'black', 'equipment', 'fashion', 'skin', 'face', 'headgear', 'jaw', 'people', 'head', 'beard', 'fun', 'orange', 'active', 'cheek', 'forehead', 'sleeve', 'lips', 'shirt', 'vision', 'chin', 'glasses', 'eyewear', 'care', 'hair', 'eyebrow', 'expression', 'facial', 'happiness', 'smile']\n",
      "\n",
      "ðŸ”¹ Topic 4 top words:\n",
      "['electrical', 'electricity', 'overhead', 'cable', 'power', 'camera', 'cameras', 'optics', 'aerospace', 'aviation', 'fixed', 'digital', 'reflex', 'manufacturer', 'military', 'engineering', 'air', 'travel', 'film', 'operator', 'bowl', 'super', 'american', 'trucker', 'cricket', 'cuisine', 'italian', 'establishment', 'sun', 'aircraft', 'pizza', 'leaf', 'vegetable', 'party', 'tobacco', 'baseball', 'rapper', 'neighbourhood', 'window', 'facade', 'town', 'area', 'urban', 'wool', 'woolen', 'knit', 'branch', 'grass', 'spring', 'varnish', 'plantation', 'agriculture', 'greenhouse', 'menu', 'piece', 'balloon', 'garment', 'frame', 'picture', 'shock', 'absorber', 'motorcycle', 'fuel', 'motorcycling', 'tank', 'cigarette', 'products', 'smoking', 'rubble', 'bedrock', 'dessert', 'shopkeeper', 'countertop', 'curl', 'jheri', 'cannoli', 'pastry', 'bakery', 'teacup', 'herb', 'bird', 'chicken', 'comb', 'phasianidae', 'fowl', 'damper', 'bread', 'baking', 'kettlebell', 'powdered', 'sugar', 'gadget', 'dishware', 'chef', 'goggles', 'human', 'underpants', 'undergarment', 'racket', 'tennis', 'bench', 'outdoor', 'chair', 'fixture', 'wing', 'electronic', 'cap', 'workwear', 'visibility', 'sketch', 'yoga', 'soft', 'racquet', 'racketlon', 'picnic', 'table', 'skateboarding', 'natural', 'supply', 'foods', 'medical', 'plank', 'headquarters', 'houseplant', 'hospital', 'veterans', 'signage', 'bed', 'patient', 'tight', 'knee', 'skateboard', 'law', 'enforcement', 'bridle', 'footwear', 'horse', 'riding', 'english', 'ball', 'soccer', 'denim', 'nail', 'bottle', 'midnight', 'mustelids', 'marsupial', 'performance', 'mat', 'pants', 'performing', 'hand', 'corporate', 'hardwood', 'instrument', 'monochrome', 'musical', 'emblem', 'salon', 'fast', 'log', 'cabin', 'brambles', 'berry', 'couch', 'coach', 'professional', 'crossfit', 'signature', 'event', 'blossom', 'drawing', 'floor', 'sneakers', 'flat', 'shorts', 'dusk', 'calf', 'jean', 'wire', 'dance', 'cut', 'candle', 'chin', 'cheek', 'floral', 'face', 'entertainment', 'choreography', 'pool', 'resort', 'swimming', 'decal', 'dress', 'customer', 'jewellery', 'eyebrow', 'ingredient', 'skin', 'photograph', 'flooring', 'forehead', 'football', 'group', 'case', 'friendship', 'guitarist', 'string', 'guitar', 'chordophone', 'bag', 'waste', 'holder', 'wax', 'crown', 'headpiece', 'supplies', 'keg', 'barrel', 'cylinder', 'brewery', 'watercraft', 'sail', 'sailboat', 'boat', 'sailing', 'pleased', 'venue', 'serveware', 'head', 'audio', 'ornament', 'symmetry', 'scholar', 'barefoot', 'toe', 'heart', 'dancer', 'ballet', 'banner', 'kit', 'hairstyle', 'crew', 'comfort', 'interior', 'building', 'desk', 'buzz', 'transparency', 'cartilaginous', 'fishes', 'fin', 'shark', 'outerwear', 'floristry', 'flower', 'organization', 'kneeling', 'afro', 'retail', 'moustache', 'community', 'lotus', 'triangle', 'contact', 'training', 'scrap', 'rust', 'motel', 'siding', 'inn', 'beauty', 'people', 'lips', 'cup', 'coffee', 'classic', 'high', 'winter', 'laughter', 'news', 'smoke', 'shoulder', 'fitness', 'nature', 'composite', 'sign', 'physical', 'polo', 'sitting', 'pixie', 'portrait', 'day', 'produce', 'line', 'clothing', 'brickwork', 'social', 'collision', 'traffic', 'flowerpot', 'arm', 'phenomenon', 'geological', 'petal', 'ceiling', 'waist', 'artist', 'tattoo', 'child', 'room', 'material', 'neon', 'plastic', 'red', 'eyelash', 'plywood', 'molding', 'collage', 'aluminium', 'glass', 'darkness', 'cheese', 'style', 'vacation', 'fish', 'love', 'flame', 'pub', 'photography', 'jacket', 'headphones', 'bar', 'billboard', 'sticker', 'fence', 'handrail', 'brick', 'kindergarten', 'temple', 'dreadlocks', 'cleanliness', 'spa', 'eyewear', 'glasses', 'vision', 'shed', 'restaurant', 'finger', 'headgear', 'fun', 'service', 'emergency', 'thumb', 'wrist', 'muscle', 'wood', 'label', 'care', 'jaw', 'light', 'machine', 'pink', 'shelving', 'brake', 'tail', 'sleeve', 'sport', 'awning', 'food', 'hat', 'equipment', 'department', 'neck', 'text', 'graphic', 'happiness', 'orange', 'stain', 'exercise', 'shelf', 'sunglasses', 'advertising', 'device', 'purple', 'iron', 'metal', 'logo', 'graphics', 'shirt', 'night', 'symbol', 'design', 'utility', 'leisure', 'furniture', 'clip', 'lighting', 'display', 'terrestrial', 'caption', 'facial', 'luxury', 'mouth', 'expression', 'road', 'active', 'surface', 'lane', 'white', 'software', 'game', 'video', 'modern', 'painting', 'asphalt', 'paper', 'family', 'basket', 'wicker', 'product', 'sleeveless', 'yellow', 'door', 'animal', 'bicycle', 'badge', 'jeans', 'smile', 'drinking', 'fashion', 'commercial', 'working', 'team', 'collar', 'garden', 'personal', 'weights', 'daytime', 'gym', 'blond', 'beard', 'california', 'health', 'milkshake', 'straw', 'shake', 'bone', 'skeleton', 'skull', 'album', 'model', 'sports', 'cooking', 'government', 'spokesperson', 'youth', 'jersey', 'selfie', 'domestic', 'suidae', 'pig', 'meat', 'tool', 'tradesman', 'soil', 'states', 'united', 'cook', 'apron', 'drinkware', 'balance', 'woodland', 'lake', 'channel', 'reservoir', 'bank', 'reflection', 'caravan', 'recreational', 'appliance', 'kitchen', 'telephony', 'drink', 'alcoholic', 'stemware', 'linens', 'silver', 'toy', 'education', 'science', 'bumper', 'shadow', 'cosmetics', 'wig', 'earring', 'jewelry', 'shoot', 'institution', 'secondary', 'mirror', 'steering', 'driving', 'rear', 'view', 'body', 'phone', 'mobile', 'motif', 'textile', 'thigh', 'foot', 'tableware', 'academic', 'student', 'truck', 'parking', 'poster', 'photo', 'black', 'elbow', 'tooth', 'standing', 'wear', 'formal', 'makeover', 'handwriting', 'bookcase', 'worker', 'costume', 'cover', 'illustration', 'canidae', 'flag', 'vertebrate', 'street', 'uniform', 'van', 'barware', 'recreation', 'wall', 'long', 'eye', 'publication', 'whiskers', 'mural', 'graffiti', 'school', 'creative', 'paint', 'screenshot', 'trees', 'snout', 'visual', 'brown', 'felinae', 'felidae', 'cat', 'hair', 'wheel', 'font', 'nose', 'book', 'palm', 'animated', 'land', 'art', 'character', 'fictional', 'transport', 'mode', 'exterior', 'dog', 'arts', 'animation', 'car', 'cartoon', 'carnivores', 'motor', 'tire', 'automotive', 'blue', 'vehicle']\n",
      "\n",
      "ðŸ”¹ Topic 5 top words:\n",
      "['american', 'bowl', 'super', 'land', 'view', 'mirror', 'steering', 'driving', 'rear', 'tobacco', 'party', 'institution', 'secondary', 'shoot', 'smoking', 'products', 'cigarette', 'rubble', 'bedrock', 'jheri', 'curl', 'motorcycling', 'motorcycle', 'shock', 'fuel', 'absorber', 'tank', 'menu', 'piece', 'garment', 'balloon', 'bumper', 'jewelry', 'shadow', 'earring', 'wig', 'cosmetics', 'dessert', 'uniform', 'graffiti', 'barware', 'countertop', 'shopkeeper', 'soil', 'tradesman', 'tool', 'transport', 'mode', 'van', 'frame', 'picture', 'department', 'undergarment', 'human', 'underpants', 'kettlebell', 'herb', 'teacup', 'cannoli', 'pastry', 'street', 'bakery', 'toy', 'education', 'science', 'linens', 'silver', 'stemware', 'alcoholic', 'drink', 'spokesperson', 'government', 'mural', 'animated', 'sketch', 'tennis', 'racket', 'outdoor', 'bench', 'telephony', 'chair', 'thumb', 'cartoon', 'powdered', 'sugar', 'fictional', 'character', 'vertebrate', 'kitchen', 'appliance', 'yoga', 'racketlon', 'racquet', 'soft', 'picnic', 'table', 'skateboarding', 'veterans', 'marsupial', 'mustelids', 'canidae', 'natural', 'woodland', 'channel', 'reservoir', 'reflection', 'lake', 'bank', 'suidae', 'pig', 'domestic', 'meat', 'visual', 'medical', 'illustration', 'foods', 'plank', 'publication', 'headquarters', 'hospital', 'tight', 'jersey', 'youth', 'houseplant', 'patient', 'bed', 'performance', 'performing', 'snout', 'footwear', 'riding', 'bridle', 'horse', 'english', 'ball', 'soccer', 'knee', 'skateboard', 'nail', 'bottle', 'mat', 'pants', 'cover', 'musical', 'recreational', 'caravan', 'book', 'hand', 'instrument', 'corporate', 'monochrome', 'hardwood', 'emblem', 'salon', 'model', 'dance', 'cabin', 'log', 'blossom', 'signature', 'couch', 'coach', 'crossfit', 'professional', 'flat', 'berry', 'brambles', 'floor', 'choreography', 'sneakers', 'wire', 'entertainment', 'event', 'cut', 'drawing', 'candle', 'calf', 'shorts', 'jean', 'dusk', 'chin', 'floral', 'cheek', 'jewellery', 'face', 'dress', 'terrestrial', 'worker', 'muscle', 'body', 'swimming', 'pool', 'resort', 'flag', 'football', 'flooring', 'photograph', 'jaw', 'stain', 'iron', 'group', 'symmetry', 'ornament', 'guitarist', 'guitar', 'string', 'chordophone', 'bag', 'waste', 'dancer', 'ballet', 'holder', 'wax', 'venue', 'supplies', 'crown', 'headpiece', 'friendship', 'cylinder', 'keg', 'barrel', 'brewery', 'watercraft', 'boat', 'sail', 'sailboat', 'sailing', 'scholar', 'classic', 'pleased', 'barefoot', 'toe', 'head', 'software', 'game', 'metal', 'heart', 'clip', 'community', 'costume', 'balance', 'bone', 'skeleton', 'skull', 'hairstyle', 'desk', 'painting', 'modern', 'united', 'states', 'road', 'building', 'long', 'album', 'afro', 'mobile', 'phone', 'caption', 'transparency', 'cartilaginous', 'fin', 'shark', 'fishes', 'outerwear', 'flower', 'floristry', 'neck', 'buzz', 'bookcase', 'exercise', 'drinkware', 'kneeling', 'organization', 'moustache', 'beauty', 'lotus', 'triangle', 'training', 'contact', 'rust', 'scrap', 'coffee', 'cup', 'wall', 'laughter', 'news', 'motel', 'siding', 'inn', 'animal', 'wood', 'surface', 'lane', 'winter', 'academic', 'fitness', 'foot', 'wear', 'formal', 'bicycle', 'mouth', 'animation', 'composite', 'photo', 'physical', 'motif', 'sitting', 'polo', 'day', 'school', 'pixie', 'portrait', 'sports', 'student', 'badge', 'daytime', 'brickwork', 'social', 'eye', 'poster', 'team', 'thigh', 'blond', 'tattoo', 'artist', 'arm', 'phenomenon', 'geological', 'petal', 'child', 'waist', 'flowerpot', 'material', 'machine', 'room', 'plywood', 'makeover', 'eyelash', 'photography', 'molding', 'collage', 'aluminium', 'glass', 'tableware', 'standing', 'weights', 'elbow', 'nose', 'text', 'fish', 'screenshot', 'love', 'flame', 'wicker', 'basket', 'working', 'gym', 'jeans', 'commercial', 'tooth', 'brown', 'pub', 'handrail', 'fence', 'kindergarten', 'brick', 'dreadlocks', 'spa', 'personal', 'eyebrow', 'happiness', 'sleeveless', 'shed', 'graphics', 'finger', 'forehead', 'glasses', 'vision', 'expression', 'red', 'graphic', 'trees', 'paint', 'blue', 'art', 'skin', 'shoulder', 'label', 'lips', 'shelf', 'fashion', 'carnivores', 'facial', 'palm', 'furniture', 'hair', 'pink', 'collar', 'dog', 'arts', 'symbol', 'clothing', 'shelving', 'plastic', 'felidae', 'cat', 'felinae', 'design', 'font', 'smoke', 'shirt', 'interior', 'customer', 'banner', 'retail', 'emergency', 'whiskers', 'sleeve', 'motor', 'wrist', 'workwear', 'visibility', 'high', 'care', 'orange', 'advertising', 'creative', 'textile', 'active', 'white', 'smile', 'black', 'kit', 'equipment', 'service', 'recreation', 'decal', 'handwriting', 'straw', 'milkshake', 'shake', 'health', 'sport', 'awning', 'vehicle', 'car', 'audio', 'traffic', 'collision', 'yellow', 'darkness', 'neon', 'cleanliness', 'family', 'door', 'utility', 'headphones', 'style', 'cheese', 'california', 'tire', 'logo', 'wheel', 'leisure', 'temple', 'billboard', 'sticker', 'product', 'ingredient', 'people', 'denim', 'apron', 'baking', 'damper', 'bread', 'goggles', 'dishware', 'chef', 'chicken', 'bird', 'fowl', 'phasianidae', 'comb', 'varnish', 'grass', 'spring', 'branch', 'knit', 'wool', 'woolen', 'agriculture', 'plantation', 'greenhouse', 'rapper', 'neighbourhood', 'area', 'town', 'window', 'facade', 'urban', 'sun', 'establishment', 'cuisine', 'italian', 'cricket', 'trucker', 'operator', 'film', 'travel', 'engineering', 'air', 'military', 'manufacturer', 'reflex', 'overhead', 'cable', 'power', 'electricity', 'selfie', 'serveware', 'cook', 'supply', 'enforcement', 'law', 'asphalt', 'produce', 'crew', 'video', 'nature', 'vacation', 'line', 'garden', 'bar', 'eyewear', 'beard', 'headgear', 'ceiling', 'sign', 'drinking', 'jacket', 'case', 'paper', 'luxury', 'fixture', 'gadget', 'fast', 'vegetable', 'leaf', 'digital', 'aviation', 'fixed', 'aerospace', 'electrical', 'cooking', 'fun', 'truck', 'display', 'comfort', 'wing', 'signage', 'cameras', 'optics', 'exterior', 'purple', 'midnight', 'pizza', 'baseball', 'restaurant', 'lighting', 'parking', 'sunglasses', 'device', 'tail', 'brake', 'aircraft', 'electronic', 'camera', 'hat', 'cap', 'night', 'food', 'light', 'automotive']\n"
     ]
    }
   ],
   "source": [
    "for i, topic in enumerate(lda.components_):\n",
    "    top_word_list = [words[j] for j in topic.argsort()]  # Top 15 words per topic\n",
    "    topic_words.append({\n",
    "        \"Topic\": f\"Topic_{i+1}\",\n",
    "        \"Top_Words\": \", \".join(top_word_list)\n",
    "    })\n",
    "    print(f\"\\nðŸ”¹ Topic {i+1} top words:\")\n",
    "    print(top_word_list)\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "df_topic_words1 = pd.DataFrame(topic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Topic                                                  Topic_1\n",
       "Top_Words    chair, handwriting, interior, facial, hairstyl...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_words1.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word_lists = []\n",
    "for i, topic in enumerate(lda.components_):\n",
    "    sorted_indices = topic.argsort()[::-1]          # sort words by probability (high â†’ low)\n",
    "    topic_words = [words[j] for j in sorted_indices] # all words in order\n",
    "    topic_word_lists.append(topic_words)\n",
    "\n",
    "# --- Make them into columns of equal length (pad shorter lists with blanks) ---\n",
    "max_len = max(len(t) for t in topic_word_lists)\n",
    "topic_columns = {}\n",
    "\n",
    "for i, word_list in enumerate(topic_word_lists):\n",
    "    padded_list = word_list + [\"\"] * (max_len - len(word_list))\n",
    "    topic_columns[f\"Topic_{i+1}\"] = padded_list\n",
    "\n",
    "# --- Create DataFrame ---\n",
    "df_topic_columns = pd.DataFrame(topic_columns)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_1</th>\n",
       "      <th>Topic_2</th>\n",
       "      <th>Topic_3</th>\n",
       "      <th>Topic_4</th>\n",
       "      <th>Topic_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>food</td>\n",
       "      <td>design</td>\n",
       "      <td>smile</td>\n",
       "      <td>vehicle</td>\n",
       "      <td>automotive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>beauty</td>\n",
       "      <td>graphics</td>\n",
       "      <td>happiness</td>\n",
       "      <td>blue</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>restaurant</td>\n",
       "      <td>graphic</td>\n",
       "      <td>facial</td>\n",
       "      <td>automotive</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>furniture</td>\n",
       "      <td>logo</td>\n",
       "      <td>expression</td>\n",
       "      <td>tire</td>\n",
       "      <td>night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>customer</td>\n",
       "      <td>font</td>\n",
       "      <td>eyebrow</td>\n",
       "      <td>motor</td>\n",
       "      <td>cap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>overhead</td>\n",
       "      <td>overhead</td>\n",
       "      <td>cable</td>\n",
       "      <td>power</td>\n",
       "      <td>view</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>electricity</td>\n",
       "      <td>cable</td>\n",
       "      <td>power</td>\n",
       "      <td>cable</td>\n",
       "      <td>land</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>power</td>\n",
       "      <td>power</td>\n",
       "      <td>electricity</td>\n",
       "      <td>overhead</td>\n",
       "      <td>super</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>cable</td>\n",
       "      <td>electricity</td>\n",
       "      <td>overhead</td>\n",
       "      <td>electricity</td>\n",
       "      <td>bowl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>electrical</td>\n",
       "      <td>electrical</td>\n",
       "      <td>electrical</td>\n",
       "      <td>electrical</td>\n",
       "      <td>american</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>583 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Topic_1      Topic_2      Topic_3      Topic_4     Topic_5\n",
       "0           food       design        smile      vehicle  automotive\n",
       "1         beauty     graphics    happiness         blue       light\n",
       "2     restaurant      graphic       facial   automotive        food\n",
       "3      furniture         logo   expression         tire       night\n",
       "4       customer         font      eyebrow        motor         cap\n",
       "..           ...          ...          ...          ...         ...\n",
       "578     overhead     overhead        cable        power        view\n",
       "579  electricity        cable        power        cable        land\n",
       "580        power        power  electricity     overhead       super\n",
       "581        cable  electricity     overhead  electricity        bowl\n",
       "582   electrical   electrical   electrical   electrical    american\n",
       "\n",
       "[583 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_columns.to_csv(\"lda_topic_words1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Average Topic Weights by Quartile:\n",
      "         High (Top 25%)  Low (Bottom 25%)  Difference\n",
      "Topic_1           0.156             0.196      -0.039\n",
      "Topic_2           0.262             0.217       0.045\n",
      "Topic_3           0.300             0.213       0.087\n",
      "Topic_4           0.152             0.195      -0.043\n",
      "Topic_5           0.130             0.179      -0.049\n",
      "\n",
      "âœ… Saved:\n",
      "â€¢ TaskE_word_topic_matrix.csv  â†’ topic weights per image\n",
      "â€¢ TaskE_topic_quartile_summary.csv  â†’ high vs low quartile comparison\n"
     ]
    }
   ],
   "source": [
    "# --- Task E: Topic Modeling on Image Labels and Quartile Analysis ---\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# 1ï¸âƒ£ Prepare the text column\n",
    "df_labeled['labels'] = df_labeled['labels'].fillna(\"\")\n",
    "\n",
    "# 2ï¸âƒ£ Bag of Words for image labels\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(df_labeled['labels'])\n",
    "\n",
    "# 3ï¸âƒ£ Fit LDA model  (start with 5 topics â€“ adjust if needed)\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "topic_matrix = lda.fit_transform(X)\n",
    "\n",
    "# 4ï¸âƒ£ Add topic weights to the dataframe\n",
    "topic_cols = [f\"Topic_{i+1}\" for i in range(lda.n_components)]\n",
    "df_topics = pd.concat([df_labeled.reset_index(drop=True),\n",
    "                       pd.DataFrame(topic_matrix, columns=topic_cols)], axis=1)\n",
    "\n",
    "# 5ï¸âƒ£ Sort by likes (amount_raised or likes column)\n",
    "df_sorted = df_topics.sort_values('amount_raised', ascending=False)\n",
    "\n",
    "# 6ï¸âƒ£ Compute quartiles\n",
    "q1 = df_sorted['amount_raised'].quantile(0.25)\n",
    "q3 = df_sorted['amount_raised'].quantile(0.75)\n",
    "\n",
    "top_quartile = df_sorted[df_sorted['amount_raised'] >= q3]\n",
    "bottom_quartile = df_sorted[df_sorted['amount_raised'] <= q1]\n",
    "\n",
    "# 7ï¸âƒ£ Average topic weights for each quartile\n",
    "high_avg = top_quartile[topic_cols].mean()\n",
    "low_avg = bottom_quartile[topic_cols].mean()\n",
    "\n",
    "# 8ï¸âƒ£ Combine results in one table\n",
    "topic_comparison = pd.DataFrame({\n",
    "    'High (Top 25%)': high_avg,\n",
    "    'Low (Bottom 25%)': low_avg\n",
    "})\n",
    "topic_comparison['Difference'] = topic_comparison['High (Top 25%)'] - topic_comparison['Low (Bottom 25%)']\n",
    "\n",
    "print(\"\\nðŸ“Š Average Topic Weights by Quartile:\")\n",
    "print(topic_comparison.round(3))\n",
    "\n",
    "# 9ï¸âƒ£ Save both the full topic-weight matrix and summary table\n",
    "df_topics.to_csv(\"TaskE_word_topic_matrix.csv\", index=False)\n",
    "topic_comparison.to_csv(\"TaskE_topic_quartile_summary.csv\")\n",
    "\n",
    "print(\"\\nâœ… Saved:\")\n",
    "print(\"â€¢ TaskE_word_topic_matrix.csv  â†’ topic weights per image\")\n",
    "print(\"â€¢ TaskE_topic_quartile_summary.csv  â†’ high vs low quartile comparison\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved as 'TaskE_word_topic_matrix1.csv'\n",
      "Columns: word, Topic_1, Topic_2, Topic_3, Topic_4\n"
     ]
    }
   ],
   "source": [
    "# --- Word-Topic Probability Matrix (for manual topic naming) ---\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# 1ï¸âƒ£ Prepare text data\n",
    "df_labeled['labels'] = df_labeled['labels'].fillna(\"\")\n",
    "\n",
    "# 2ï¸âƒ£ Bag of Words\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(df_labeled['labels'])\n",
    "words = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "# 3ï¸âƒ£ Fit LDA (4 topics in your case)\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# 4ï¸âƒ£ Get the topic-word distribution matrix\n",
    "# Shape = (n_topics, n_words)\n",
    "topic_word_matrix = lda.components_\n",
    "\n",
    "# 5ï¸âƒ£ Normalize so that probabilities sum to 1 per topic\n",
    "topic_word_probs = topic_word_matrix / topic_word_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# 6ï¸âƒ£ Transpose â†’ each row = word, each column = topic probability\n",
    "word_topic_matrix = pd.DataFrame(topic_word_probs.T, columns=[f\"Topic_{i+1}\" for i in range(lda.n_components)])\n",
    "word_topic_matrix.insert(0, \"word\", words)\n",
    "\n",
    "# 7ï¸âƒ£ Save to CSV\n",
    "word_topic_matrix.to_csv(\"TaskE_word_topic_matrix2.csv\", index=False)\n",
    "print(\"âœ… Saved as 'TaskE_word_topic_matrix1.csv'\")\n",
    "print(\"Columns: word, Topic_1, Topic_2, Topic_3, Topic_4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved file: TaskF_amount_topic_weights.csv\n",
      "   amount_raised    Topic1    Topic2    Topic3    Topic4\n",
      "0          18538  0.042223  0.041687  0.874380  0.041710\n",
      "1          13541  0.035821  0.892740  0.035718  0.035722\n",
      "2          35865  0.036672  0.036163  0.035732  0.891433\n",
      "3          13939  0.035837  0.469087  0.035735  0.459342\n",
      "4           5363  0.250000  0.250000  0.250000  0.250000\n",
      "5           6214  0.036007  0.291177  0.319151  0.353665\n",
      "6           8653  0.022732  0.930582  0.022729  0.023957\n",
      "7           4988  0.812375  0.062604  0.062507  0.062514\n",
      "8           4175  0.777231  0.032972  0.158519  0.031277\n",
      "9           9443  0.904030  0.033425  0.031265  0.031281\n"
     ]
    }
   ],
   "source": [
    "# --- Task F: Generate amount_raised + topic weights table ---\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# 1ï¸âƒ£ Prepare your dataset\n",
    "df_labeled['labels'] = df_labeled['labels'].fillna(\"\")\n",
    "\n",
    "# 2ï¸âƒ£ Create Bag-of-Words matrix\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(df_labeled['labels'])\n",
    "\n",
    "# 3ï¸âƒ£ Fit LDA with 4 topics\n",
    "lda = LatentDirichletAllocation(n_components=4, random_state=42)\n",
    "topic_matrix = lda.fit_transform(X)   # each row = topic distribution for one fundraiser\n",
    "\n",
    "# 4ï¸âƒ£ Combine with amount_raised\n",
    "topic_cols = [f\"Topic{i+1}\" for i in range(lda.n_components)]\n",
    "df_topics = pd.concat([\n",
    "    df_labeled[['amount_raised']].reset_index(drop=True),\n",
    "    pd.DataFrame(topic_matrix, columns=topic_cols)\n",
    "], axis=1)\n",
    "\n",
    "# 5ï¸âƒ£ Normalize topic weights to sum to 1 per row\n",
    "df_topics[topic_cols] = df_topics[topic_cols].div(df_topics[topic_cols].sum(axis=1), axis=0)\n",
    "\n",
    "# 6ï¸âƒ£ Save and display\n",
    "df_topics.to_csv(\"TaskF_amount_topic_weights.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Saved file: TaskF_amount_topic_weights.csv\")\n",
    "print(df_topics.head(10))   # show first 10 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Average Topic Weights by Quartile:\n",
      "         High $ (Top 25%)  Low $ (Bottom 25%)  Difference\n",
      "Topic_3             0.332               0.265       0.067\n",
      "Topic_2             0.315               0.272       0.043\n",
      "Topic_1             0.189               0.231      -0.042\n",
      "Topic_4             0.164               0.232      -0.068\n"
     ]
    }
   ],
   "source": [
    "# Topic Modeling and Quartile Comparison\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Prepare text data\n",
    "df_labeled['labels'] = df_labeled['labels'].fillna(\"\")\n",
    "\n",
    "# Bag of Words\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(df_labeled['labels'])\n",
    "words = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Fit LDA (4 topics)\n",
    "lda = LatentDirichletAllocation(n_components=4, random_state=42)\n",
    "topic_matrix = lda.fit_transform(X)   # document-topic distribution\n",
    "\n",
    "# Get the topic-word distribution matrix\n",
    "topic_word_matrix = lda.components_\n",
    "\n",
    "# Normalize so that probabilities sum to 1 per topic\n",
    "topic_word_probs = topic_word_matrix / topic_word_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Transpose â†’ each row = word, each column = topic probability\n",
    "word_topic_matrix = pd.DataFrame(topic_word_probs.T,\n",
    "                                 columns=[f\"Topic_{i+1}\" for i in range(lda.n_components)])\n",
    "word_topic_matrix.insert(0, \"word\", words)\n",
    "\n",
    "# Save word-topic matrix\n",
    "word_topic_matrix.to_csv(\"word_topic_matrix_final.csv\", index=False)\n",
    "\n",
    "# Quartile comparison on amount raised / likes\n",
    "\n",
    "# Combine topic weights with amount_raised\n",
    "topic_cols = [f\"Topic_{i+1}\" for i in range(lda.n_components)]\n",
    "df_topics = pd.concat([\n",
    "    df_labeled[['amount_raised']].reset_index(drop=True),\n",
    "    pd.DataFrame(topic_matrix, columns=topic_cols)\n",
    "], axis=1)\n",
    "\n",
    "# Normalize topic weights to sum to 1 per campaign\n",
    "df_topics[topic_cols] = df_topics[topic_cols].div(df_topics[topic_cols].sum(axis=1), axis=0)\n",
    "\n",
    "# Sort by actual amount raised\n",
    "df_sorted = df_topics.sort_values('amount_raised', ascending=False)\n",
    "\n",
    "# Define quartiles\n",
    "q1 = df_sorted['amount_raised'].quantile(0.25)\n",
    "q3 = df_sorted['amount_raised'].quantile(0.75)\n",
    "\n",
    "# Split into top and bottom quartiles\n",
    "top_quartile = df_sorted[df_sorted['amount_raised'] >= q3]\n",
    "bottom_quartile = df_sorted[df_sorted['amount_raised'] <= q1]\n",
    "\n",
    "# Calculate average topic weights for both quartiles\n",
    "top_avg = top_quartile[topic_cols].mean()\n",
    "bottom_avg = bottom_quartile[topic_cols].mean()\n",
    "\n",
    "# Create comparison table\n",
    "topic_comparison = pd.DataFrame({\n",
    "    'High $ (Top 25%)': top_avg,\n",
    "    'Low $ (Bottom 25%)': bottom_avg\n",
    "})\n",
    "topic_comparison['Difference'] = topic_comparison['High $ (Top 25%)'] - topic_comparison['Low $ (Bottom 25%)']\n",
    "topic_comparison = topic_comparison.sort_values('Difference', ascending=False)\n",
    "\n",
    "# Display\n",
    "print(\"\\n Average Topic Weights by Quartile:\")\n",
    "print(topic_comparison.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
